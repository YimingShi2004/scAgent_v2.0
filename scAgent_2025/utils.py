"""
Utility functions for scAgent.
"""

import re
from typing import List, Dict, Any, Optional
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

def safe_int_convert(value) -> int:
    """Safely convert a value to integer, handling strings and None."""
    if value is None:
        return 0
    if isinstance(value, int):
        return value
    if isinstance(value, str):
        value = value.strip()
        try:
            # Handle both integer and float strings
            return int(float(value))
        except (ValueError, TypeError):
            return 0
    try:
        return int(float(value))
    except (ValueError, TypeError):
        return 0

def generate_sra_download_commands(sra_records: List[Dict[str, Any]]) -> List[str]:
    """
    Generate SRA download commands for a list of SRA records.
    
    Args:
        sra_records: List of SRA record dictionaries
        
    Returns:
        List of download commands
    """
    commands = []
    
    for record in sra_records:
        run_accession = record.get('run_accession', '')
        if run_accession:
            # Generate prefetch command
            prefetch_cmd = f"prefetch {run_accession}"
            commands.append(prefetch_cmd)
            
            # Generate fastq-dump command
            fastq_cmd = f"fastq-dump --split-files --gzip {run_accession}"
            commands.append(fastq_cmd)
            
            # Generate fasterq-dump command (faster alternative)
            fasterq_cmd = f"fasterq-dump --split-files --progress {run_accession} && gzip *.fastq"
            commands.append(f"# Alternative: {fasterq_cmd}")
    
    return commands

def generate_sra_download_script(
    sra_records: List[Dict[str, Any]], 
    output_file: str = "download_sra.sh"
) -> str:
    """
    Generate a shell script for downloading SRA data.
    
    Args:
        sra_records: List of SRA record dictionaries
        output_file: Output script filename
        
    Returns:
        Path to the generated script
    """
    commands = generate_sra_download_commands(sra_records)
    
    script_content = """#!/bin/bash
# SRA Data Download Script
# Generated by scAgent

set -e  # Exit on error

echo "Starting SRA data download..."
echo "Total datasets: {total_datasets}"
echo ""

# Create output directory
mkdir -p sra_data
cd sra_data

# Download commands
{download_commands}

echo ""
echo "Download complete!"
echo "Files are located in: $(pwd)"
""".format(
        total_datasets=len(sra_records),
        download_commands="\n".join(commands)
    )
    
    with open(output_file, 'w') as f:
        f.write(script_content)
    
    # Make script executable
    import os
    os.chmod(output_file, 0o755)
    
    logger.info(f"Generated download script: {output_file}")
    return output_file

def validate_sra_accession(accession: str) -> bool:
    """
    Validate SRA accession format.
    
    Args:
        accession: SRA accession string
        
    Returns:
        True if valid, False otherwise
    """
    # SRA accession patterns
    patterns = [
        r'^SRR\d+$',  # Run accession
        r'^SRS\d+$',  # Sample accession
        r'^SRX\d+$',  # Experiment accession
        r'^SRP\d+$',  # Study accession
    ]
    
    for pattern in patterns:
        if re.match(pattern, accession):
            return True
    
    return False

def estimate_download_size(sra_records: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Estimate total download size for SRA records.
    
    Args:
        sra_records: List of SRA record dictionaries
        
    Returns:
        Dictionary with size estimates
    """
    total_bytes = 0
    total_bases = 0
    total_spots = 0
    
    for record in sra_records:
        total_bytes += record.get('bytes', 0)
        total_bases += record.get('bases', 0)
        total_spots += record.get('spots', 0)
    
    # Convert to human readable
    def format_bytes(bytes_val):
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if bytes_val < 1024.0:
                return f"{bytes_val:.1f} {unit}"
            bytes_val /= 1024.0
        return f"{bytes_val:.1f} PB"
    
    return {
        'total_datasets': len(sra_records),
        'total_bytes': total_bytes,
        'total_bytes_formatted': format_bytes(total_bytes),
        'total_bases': total_bases,
        'total_spots': total_spots,
        'estimated_fastq_size': format_bytes(total_bytes * 3),  # FASTQ is ~3x larger than SRA
    }

def create_batch_processing_params(
    total_records: int,
    batch_size: int = 1000,
    max_workers: int = 4
) -> List[Dict[str, int]]:
    """
    Create parameters for batch processing large datasets.
    
    Args:
        total_records: Total number of records to process
        batch_size: Size of each batch
        max_workers: Maximum number of parallel workers
        
    Returns:
        List of batch parameters
    """
    batches = []
    
    for i in range(0, total_records, batch_size):
        batch = {
            'offset': i,
            'limit': min(batch_size, total_records - i),
            'batch_number': i // batch_size + 1,
            'total_batches': (total_records + batch_size - 1) // batch_size
        }
        batches.append(batch)
    
    return batches

def filter_by_quality_metrics(
    records: List[Dict[str, Any]],
    min_sample_count: int = 50,
    min_spots: int = 1000000,
    required_organisms: Optional[List[str]] = None
) -> List[Dict[str, Any]]:
    """
    Filter records by quality metrics for sc-eQTL analysis.
    
    Args:
        records: List of dataset records
        min_sample_count: Minimum sample count for GEO records
        min_spots: Minimum spots for SRA records
        required_organisms: List of required organisms
        
    Returns:
        Filtered list of records
    """
    filtered = []
    
    for record in records:
        # Check sample count for GEO records
        if 'sample_count' in record:
            if record['sample_count'] < min_sample_count:
                continue
        
        # Check spots for SRA records
        if 'spots' in record:
            if record['spots'] < min_spots:
                continue
        
        # Check organism
        if required_organisms:
            organism = record.get('organism', '')
            if organism not in required_organisms:
                continue
        
        filtered.append(record)
    
    return filtered

def generate_metadata_summary(records: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Generate a summary of metadata from records.
    
    Args:
        records: List of dataset records
        
    Returns:
        Metadata summary
    """
    summary = {
        'total_records': len(records),
        'organisms': {},
        'tissues': {},
        'platforms': {},
        'library_strategies': {},
        'date_range': {'earliest': None, 'latest': None}
    }
    
    for record in records:
        # Count organisms
        organism = record.get('organism', 'Unknown')
        summary['organisms'][organism] = summary['organisms'].get(organism, 0) + 1
        
        # Count tissues (for SRA records)
        if 'tissue' in record:
            tissue = record['tissue']
            summary['tissues'][tissue] = summary['tissues'].get(tissue, 0) + 1
        
        # Count platforms
        platform = record.get('platform', 'Unknown')
        summary['platforms'][platform] = summary['platforms'].get(platform, 0) + 1
        
        # Count library strategies (for SRA records)
        if 'library_strategy' in record:
            strategy = record['library_strategy']
            summary['library_strategies'][strategy] = summary['library_strategies'].get(strategy, 0) + 1
        
        # Track date range
        for date_field in ['submission_date', 'created_at']:
            if date_field in record and record[date_field]:
                date_val = record[date_field]
                if isinstance(date_val, str):
                    date_val = date_val.split()[0]  # Take date part only
                
                if summary['date_range']['earliest'] is None or date_val < summary['date_range']['earliest']:
                    summary['date_range']['earliest'] = date_val
                
                if summary['date_range']['latest'] is None or date_val > summary['date_range']['latest']:
                    summary['date_range']['latest'] = date_val
    
    return summary

def detect_genotype_data_availability(
    records: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Detect potential genotype data availability in dataset records.
    
    Args:
        records: List of dataset records
        
    Returns:
        Records with genotype availability assessment
    """
    genotype_keywords = [
        'genotype', 'snp', 'gwas', 'variant', 'vcf', 'wgs', 'wes',
        'whole genome', 'whole exome', 'genetic', 'allele', 'polymorphism',
        'dbgap', 'dbsnp', 'hapmap', 'qtl', 'eqtl', 'plink', 'imputation'
    ]
    
    for record in records:
        genotype_score = 0
        genotype_evidence = []
        
        # Check title and summary for genotype-related terms
        text_fields = [
            record.get('title', ''),
            record.get('summary', ''),
            record.get('overall_design', ''),
            record.get('study_title', ''),
            record.get('study_abstract', '')
        ]
        
        for field in text_fields:
            if field:
                field_lower = field.lower()
                for keyword in genotype_keywords:
                    if keyword in field_lower:
                        genotype_score += 1
                        genotype_evidence.append(f"Found '{keyword}' in text")
        
        # Check for database references
        if any(db in str(record).lower() for db in ['dbgap', 'dbsnp', 'hapmap']):
            genotype_score += 3
            genotype_evidence.append("Database reference found")
        
        # Assess genotype likelihood
        if genotype_score >= 3:
            genotype_likelihood = "High"
        elif genotype_score >= 1:
            genotype_likelihood = "Medium"
        else:
            genotype_likelihood = "Low"
        
        record['genotype_score'] = genotype_score
        record['genotype_likelihood'] = genotype_likelihood
        record['genotype_evidence'] = genotype_evidence
    
    return records

def assess_eqtl_suitability_score(
    record: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Calculate a comprehensive sc-eQTL suitability score for a dataset.
    
    Args:
        record: Dataset record
        
    Returns:
        Record with eQTL suitability assessment
    """
    score = 0
    criteria_met = []
    criteria_missing = []
    
    # Sample size assessment
    sample_count = safe_int_convert(record.get('sample_count', 0))
    if sample_count >= 100:
        score += 3
        criteria_met.append(f"Good sample size ({sample_count})")
    elif sample_count >= 50:
        score += 2
        criteria_met.append(f"Adequate sample size ({sample_count})")
    elif sample_count >= 20:
        score += 1
        criteria_met.append(f"Minimal sample size ({sample_count})")
    else:
        criteria_missing.append(f"Low sample size ({sample_count})")
    
    # Organism assessment
    organism = record.get('organism', '')
    if organism in ['Homo sapiens', 'Mus musculus']:
        score += 2
        criteria_met.append(f"Model organism ({organism})")
    elif organism:
        score += 1
        criteria_met.append(f"Known organism ({organism})")
    else:
        criteria_missing.append("No organism specified")
    
    # Single-cell technology assessment
    title = record.get('title', '').lower()
    summary = record.get('summary', '').lower()
    scrna_terms = ['single cell', 'single-cell', 'scrna', 'sc-rna', '10x', 'droplet']
    
    if any(term in title or term in summary for term in scrna_terms):
        score += 3
        criteria_met.append("Single-cell technology confirmed")
    else:
        criteria_missing.append("Single-cell technology not confirmed")
    
    # Genotype data likelihood
    genotype_likelihood = record.get('genotype_likelihood', 'Low')
    if genotype_likelihood == 'High':
        score += 4
        criteria_met.append("High genotype data likelihood")
    elif genotype_likelihood == 'Medium':
        score += 2
        criteria_met.append("Medium genotype data likelihood")
    else:
        criteria_missing.append("Low genotype data likelihood")
    
    # Platform assessment
    platform = record.get('platform', '')
    if any(tech in platform.lower() for tech in ['illumina', 'nextseq', 'hiseq', 'novaseq']):
        score += 1
        criteria_met.append(f"Standard platform ({platform})")
    
    # Calculate final grade
    if score >= 10:
        grade = "A"
        suitability = "Excellent"
    elif score >= 7:
        grade = "B"
        suitability = "Good"
    elif score >= 4:
        grade = "C"
        suitability = "Marginal"
    else:
        grade = "D"
        suitability = "Poor"
    
    record['eqtl_score'] = score
    record['eqtl_grade'] = grade
    record['eqtl_suitability'] = suitability
    record['criteria_met'] = criteria_met
    record['criteria_missing'] = criteria_missing
    
    return record

def batch_quality_assessment(
    records: List[Dict[str, Any]],
    assessment_type: str = "eqtl"
) -> Dict[str, Any]:
    """
    Perform batch quality assessment on multiple datasets.
    
    Args:
        records: List of dataset records
        assessment_type: Type of assessment ("eqtl", "general", "technical")
        
    Returns:
        Comprehensive quality assessment report
    """
    if not records:
        return {"error": "No records provided"}
    
    # Detect genotype data availability
    records = detect_genotype_data_availability(records)
    
    # Calculate eQTL suitability scores
    scored_records = []
    for record in records:
        scored_record = assess_eqtl_suitability_score(record)
        scored_records.append(scored_record)
    
    # Generate summary statistics
    total_records = len(scored_records)
    grade_distribution = {}
    genotype_distribution = {}
    organism_distribution = {}
    
    for record in scored_records:
        # Grade distribution
        grade = record.get('eqtl_grade', 'Unknown')
        grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
        
        # Genotype likelihood distribution
        genotype = record.get('genotype_likelihood', 'Unknown')
        genotype_distribution[genotype] = genotype_distribution.get(genotype, 0) + 1
        
        # Organism distribution
        organism = record.get('organism', 'Unknown')
        organism_distribution[organism] = organism_distribution.get(organism, 0) + 1
    
    # Find top candidates
    top_candidates = sorted(
        scored_records,
        key=lambda x: x.get('eqtl_score', 0),
        reverse=True
    )[:10]
    
    # Calculate quality metrics
    high_quality_count = sum(1 for r in scored_records if r.get('eqtl_grade') in ['A', 'B'])
    genotype_available_count = sum(1 for r in scored_records if r.get('genotype_likelihood') == 'High')
    
    assessment_report = {
        "total_records": total_records,
        "high_quality_count": high_quality_count,
        "high_quality_percentage": (high_quality_count / total_records) * 100 if total_records > 0 else 0,
        "genotype_available_count": genotype_available_count,
        "genotype_available_percentage": (genotype_available_count / total_records) * 100 if total_records > 0 else 0,
        "grade_distribution": grade_distribution,
        "genotype_distribution": genotype_distribution,
        "organism_distribution": organism_distribution,
        "top_candidates": top_candidates,
        "assessment_type": assessment_type,
        "timestamp": datetime.now().isoformat()
    }
    
    return assessment_report

def detect_batch_effects(
    records: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Detect potential batch effects in dataset collections.
    
    Args:
        records: List of dataset records
        
    Returns:
        Batch effect analysis report
    """
    platform_groups = {}
    date_groups = {}
    lab_groups = {}
    
    for record in records:
        # Group by platform
        platform = record.get('platform', 'Unknown')
        if platform not in platform_groups:
            platform_groups[platform] = []
        platform_groups[platform].append(record)
        
        # Group by submission date (year)
        submission_date = record.get('submission_date', '')
        if submission_date:
            # Handle both string and datetime objects
            if hasattr(submission_date, 'year'):
                # It's a datetime object
                year = str(submission_date.year)
            elif isinstance(submission_date, str) and len(submission_date) >= 4:
                # It's a string
                year = submission_date[:4]
            else:
                year = 'Unknown'
            
            if year not in date_groups:
                date_groups[year] = []
            date_groups[year].append(record)
        
        # Group by contributor/lab
        contributor = record.get('contributor', record.get('contact_name', 'Unknown'))
        if contributor not in lab_groups:
            lab_groups[contributor] = []
        lab_groups[contributor].append(record)
    
    # Analyze potential batch effects
    batch_effects = {
        "platform_diversity": len(platform_groups),
        "temporal_span": len(date_groups),
        "lab_diversity": len(lab_groups),
        "platform_groups": {k: len(v) for k, v in platform_groups.items()},
        "date_groups": {k: len(v) for k, v in date_groups.items()},
        "lab_groups": {k: len(v) for k, v in lab_groups.items()},
        "batch_risk": "Low"
    }
    
    # Assess batch risk
    if batch_effects["platform_diversity"] > 3:
        batch_effects["batch_risk"] = "High"
    elif batch_effects["platform_diversity"] > 1:
        batch_effects["batch_risk"] = "Medium"
    
    return batch_effects

def generate_sra_download_commands(
    sra_records: List[Dict[str, Any]],
    output_dir: str = "./sra_downloads"
) -> List[str]:
    """
    Generate SRA download commands for selected datasets.
    
    Args:
        sra_records: List of SRA dataset records
        output_dir: Output directory for downloads
        
    Returns:
        List of download commands
    """
    commands = []
    
    # Add directory creation command
    commands.append(f"mkdir -p {output_dir}")
    
    for record in sra_records:
        run_accession = record.get('run_accession', '')
        if run_accession:
            # Generate prefetch command
            prefetch_cmd = f"prefetch {run_accession} -O {output_dir}"
            commands.append(prefetch_cmd)
            
            # Generate fastq-dump command
            fastq_cmd = f"fastq-dump --split-files --gzip {output_dir}/{run_accession}/{run_accession}.sra -O {output_dir}"
            commands.append(fastq_cmd)
    
    return commands

def apply_sc_eqtl_filters(
    records: List[Dict[str, Any]],
    filter_config: Optional[Dict[str, Any]] = None
) -> List[Dict[str, Any]]:
    """
    Apply comprehensive sc-eQTL filtering criteria to dataset records.
    
    Args:
        records: List of dataset records
        filter_config: Configuration for filtering criteria
        
    Returns:
        Filtered list of records with sc-eQTL suitability scores
    """
    if filter_config is None:
        filter_config = {
            "required_species": ["Homo sapiens"],
            "exclude_cell_lines": True,
            "require_database_id": True,
            "require_publication": False,
            "require_sample_size": False,
            "require_country_info": False,
            "require_age_info": False,
            "require_tumor_annotation": True,
            "require_sequencing_method": True,
            "require_tissue_source": True
        }
    
    filtered_records = []
    
    for record in records:
        # Initialize filtering result
        filter_result = {
            "record": record,
            "passes_required_filters": True,
            "passes_optional_filters": True,
            "filter_scores": {},
            "filter_reasons": [],
            "overall_score": 0
        }
        
        # 1. Species filter (Required)
        species_score = check_species_filter(record, filter_config["required_species"])
        filter_result["filter_scores"]["species"] = species_score
        if species_score == 0 and filter_config["required_species"]:
            filter_result["passes_required_filters"] = False
            filter_result["filter_reasons"].append("Species not in required list")
        
        # 2. Cell line exclusion (Required)
        if filter_config["exclude_cell_lines"]:
            cell_line_score = check_cell_line_exclusion(record)
            filter_result["filter_scores"]["cell_line"] = cell_line_score
            if cell_line_score == 0:
                filter_result["passes_required_filters"] = False
                filter_result["filter_reasons"].append("Contains cell line data")
        
        # 3. Database ID availability (Required)
        if filter_config["require_database_id"]:
            db_id_score = check_database_id_availability(record)
            filter_result["filter_scores"]["database_id"] = db_id_score
            if db_id_score == 0:
                filter_result["passes_required_filters"] = False
                filter_result["filter_reasons"].append("No valid database ID")
        
        # 4. Publication info (Optional)
        if filter_config["require_publication"]:
            pub_score = check_publication_info(record)
            filter_result["filter_scores"]["publication"] = pub_score
            if pub_score == 0:
                filter_result["passes_optional_filters"] = False
                filter_result["filter_reasons"].append("No publication information")
        
        # 5. Sample size info (Optional)
        if filter_config["require_sample_size"]:
            sample_size_score = check_sample_size_info(record)
            filter_result["filter_scores"]["sample_size"] = sample_size_score
            if sample_size_score == 0:
                filter_result["passes_optional_filters"] = False
                filter_result["filter_reasons"].append("No sample size information")
        
        # 6. Country information (Optional)
        if filter_config["require_country_info"]:
            country_score = check_country_info(record)
            filter_result["filter_scores"]["country"] = country_score
            if country_score == 0:
                filter_result["passes_optional_filters"] = False
                filter_result["filter_reasons"].append("No country information")
        
        # 7. Age information (Optional)
        if filter_config["require_age_info"]:
            age_score = check_age_info(record)
            filter_result["filter_scores"]["age"] = age_score
            if age_score == 0:
                filter_result["passes_optional_filters"] = False
                filter_result["filter_reasons"].append("No age information")
        
        # 8. Tumor annotation (Required)
        if filter_config["require_tumor_annotation"]:
            tumor_score = check_tumor_annotation(record)
            filter_result["filter_scores"]["tumor"] = tumor_score
            if tumor_score == 0:
                filter_result["passes_required_filters"] = False
                filter_result["filter_reasons"].append("No tumor status annotation")
        
        # 9. Sequencing method (Required)
        if filter_config["require_sequencing_method"]:
            seq_method_score = check_sequencing_method(record)
            filter_result["filter_scores"]["sequencing_method"] = seq_method_score
            if seq_method_score == 0:
                filter_result["passes_required_filters"] = False
                filter_result["filter_reasons"].append("No clear sequencing method")
        
        # 10. Tissue source (Required)
        if filter_config["require_tissue_source"]:
            tissue_score = check_tissue_source(record)
            filter_result["filter_scores"]["tissue"] = tissue_score
            if tissue_score == 0:
                filter_result["passes_required_filters"] = False
                filter_result["filter_reasons"].append("No clear tissue source")
        
        # Calculate overall score
        filter_result["overall_score"] = sum(filter_result["filter_scores"].values())
        
        # Add filter result to record
        record["sc_eqtl_filter_result"] = filter_result
        
        # Only include records that pass required filters
        if filter_result["passes_required_filters"]:
            filtered_records.append(record)
    
    return filtered_records

def check_species_filter(record: Dict[str, Any], required_species: List[str]) -> int:
    """Check if record contains required species."""
    organism = record.get('organism', '').lower()
    title = record.get('title', '').lower()
    summary = record.get('summary', '').lower()
    
    for species in required_species:
        species_lower = species.lower()
        if (species_lower in organism or 
            species_lower in title or 
            species_lower in summary):
            return 2  # High score for required species
    
    return 0  # No required species found

def check_cell_line_exclusion(record: Dict[str, Any]) -> int:
    """Check if record excludes cell lines."""
    cell_line_keywords = [
        'hela', '293t', 'k562', 'jurkat', 'hek293', 'mcf7', 'a549',
        'cell line', 'cell-line', 'immortalized', 'transformed'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('cell_type', '').lower(),
        record.get('source_name', '').lower()
    ]
    
    for text in text_fields:
        for keyword in cell_line_keywords:
            if keyword in text:
                return 0  # Cell line detected
    
    return 2  # No cell line detected

def check_database_id_availability(record: Dict[str, Any]) -> int:
    """Check if record has valid database IDs."""
    # Check GEO accession - use correct field name 'gse'
    geo_accession = (record.get('gse') or '').strip()
    
    # Check SRA accession - use correct field names
    sra_accession = (record.get('run_accession') or record.get('study_accession') or '').strip()
    
    if geo_accession and geo_accession.startswith('GSE'):
        return 2  # Valid GEO ID
    elif sra_accession and (sra_accession.startswith('SRR') or sra_accession.startswith('SRP') or 
                           sra_accession.startswith('ERR') or sra_accession.startswith('DRR')):
        return 2  # Valid SRA ID (including European and Japanese archives)
    
    return 0  # No valid database ID

def check_publication_info(record: Dict[str, Any]) -> int:
    """Check if record has publication information."""
    pmid = record.get('pmid', '')
    doi = record.get('doi', '')
    pubmed_id = record.get('pubmed_id', '')
    
    if pmid or doi or pubmed_id:
        return 1  # Has publication info
    
    # Check in title/summary for publication keywords
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower()
    ]
    
    pub_keywords = ['pmid', 'doi', 'pubmed', 'published', 'paper', 'article']
    
    for text in text_fields:
        for keyword in pub_keywords:
            if keyword in text:
                return 1  # Likely has publication info
    
    return 0  # No publication info

def check_sample_size_info(record: Dict[str, Any]) -> int:
    """Check if record has sample size information."""
    sample_count = safe_int_convert(record.get('sample_count', 0))
    
    if sample_count and sample_count > 0:
        if sample_count >= 100:
            return 2  # Large sample size
        elif sample_count >= 20:
            return 1  # Adequate sample size
        else:
            return 0  # Small sample size
    
    # Check in text for sample size mentions
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower()
    ]
    
    import re
    for text in text_fields:
        # Look for patterns like "n=50", "50 samples", "50 subjects"
        matches = re.findall(r'(?:n\s*=\s*|samples?\s*=\s*|subjects?\s*=\s*|patients?\s*=\s*)?(\d+)\s*(?:samples?|subjects?|patients?|individuals?)', text)
        if matches:
            max_size = max(int(match) for match in matches)
            if max_size >= 100:
                return 2
            elif max_size >= 20:
                return 1
    
    return 0  # No sample size info

def check_country_info(record: Dict[str, Any]) -> int:
    """Check if record has country/geographic information."""
    country_keywords = [
        'usa', 'united states', 'china', 'uk', 'united kingdom', 'germany', 
        'japan', 'france', 'canada', 'australia', 'sweden', 'netherlands',
        'country', 'nation', 'geographic', 'location', 'region'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('country', '').lower(),
        record.get('geographic_location', '').lower()
    ]
    
    for text in text_fields:
        for keyword in country_keywords:
            if keyword in text:
                return 1  # Has country info
    
    return 0  # No country info

def check_age_info(record: Dict[str, Any]) -> int:
    """Check if record has age information."""
    age_keywords = [
        'age', 'years old', 'year-old', 'adult', 'pediatric', 'elderly',
        'infant', 'child', 'adolescent', 'birth', 'born'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('age', '').lower(),
        record.get('characteristics', '').lower()
    ]
    
    import re
    for text in text_fields:
        # Look for age patterns
        age_patterns = [
            r'\b\d+\s*(?:years?\s*old|y\.?o\.?|yr\.?s?)\b',
            r'\b(?:age|aged)\s*\d+',
            r'\b\d+\s*-\s*\d+\s*(?:years?|y\.?o\.?)'
        ]
        
        for pattern in age_patterns:
            if re.search(pattern, text):
                return 1  # Has age info
        
        for keyword in age_keywords:
            if keyword in text:
                return 1  # Has age-related info
    
    return 0  # No age info

def check_tumor_annotation(record: Dict[str, Any]) -> int:
    """Check if record has tumor status annotation."""
    tumor_keywords = [
        'tumor', 'tumour', 'cancer', 'carcinoma', 'adenocarcinoma', 'malignant',
        'normal', 'healthy', 'control', 'non-tumor', 'benign'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('tissue', '').lower(),
        record.get('disease', '').lower(),
        record.get('characteristics', '').lower()
    ]
    
    for text in text_fields:
        for keyword in tumor_keywords:
            if keyword in text:
                return 1  # Has tumor status annotation
    
    return 0  # No tumor status annotation

def check_sequencing_method(record: Dict[str, Any]) -> int:
    """Check if record has clear sequencing method."""
    sequencing_methods = [
        '10x genomics', '10x', 'smart-seq', 'smart-seq2', 'drop-seq',
        'cel-seq', 'mars-seq', 'scrb-seq', 'plate-seq', 'well-seq',
        'single-cell', 'single cell', 'sc-rna', 'scrna'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('library_strategy', '').lower(),
        record.get('platform', '').lower(),
        record.get('instrument', '').lower()
    ]
    
    for text in text_fields:
        for method in sequencing_methods:
            if method in text:
                return 2  # Clear sequencing method
    
    return 0  # No clear sequencing method

def check_tissue_source(record: Dict[str, Any]) -> int:
    """Check if record has clear tissue source."""
    tissue_keywords = [
        'brain', 'liver', 'heart', 'lung', 'kidney', 'muscle', 'blood',
        'skin', 'bone', 'pancreas', 'stomach', 'intestine', 'colon',
        'breast', 'ovary', 'testis', 'prostate', 'thyroid', 'spleen',
        'tissue', 'organ', 'biopsy', 'sample'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('tissue', '').lower(),
        record.get('organ', '').lower(),
        record.get('source_name', '').lower()
    ]
    
    for text in text_fields:
        for keyword in tissue_keywords:
            if keyword in text:
                return 1  # Has tissue source
    
    return 0  # No clear tissue source

def generate_filter_report(
    original_records: List[Dict[str, Any]],
    filtered_records: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """Generate comprehensive filtering report."""
    
    report = {
        "filtering_summary": {
            "original_count": len(original_records),
            "filtered_count": len(filtered_records),
            "retention_rate": (len(filtered_records) / len(original_records)) * 100 if original_records else 0
        },
        "filter_statistics": {},
        "quality_distribution": {},
        "common_rejection_reasons": [],
        "timestamp": datetime.now().isoformat()
    }
    
    # Analyze filter statistics
    filter_categories = [
        "species", "cell_line", "database_id", "publication", "sample_size",
        "country", "age", "tumor", "sequencing_method", "tissue"
    ]
    
    for category in filter_categories:
        scores = []
        for record in original_records:
            filter_result = record.get("sc_eqtl_filter_result", {})
            score = filter_result.get("filter_scores", {}).get(category, 0)
            scores.append(score)
        
        if scores:
            report["filter_statistics"][category] = {
                "average_score": sum(scores) / len(scores),
                "pass_count": sum(1 for s in scores if s > 0),
                "fail_count": sum(1 for s in scores if s == 0),
                "pass_rate": (sum(1 for s in scores if s > 0) / len(scores)) * 100
            }
    
    # Analyze quality distribution
    quality_scores = []
    for record in filtered_records:
        filter_result = record.get("sc_eqtl_filter_result", {})
        score = filter_result.get("overall_score", 0)
        quality_scores.append(score)
    
    if quality_scores:
        report["quality_distribution"] = {
            "average_score": sum(quality_scores) / len(quality_scores),
            "min_score": min(quality_scores),
            "max_score": max(quality_scores),
            "high_quality_count": sum(1 for s in quality_scores if s >= 8),
            "medium_quality_count": sum(1 for s in quality_scores if 4 <= s < 8),
            "low_quality_count": sum(1 for s in quality_scores if s < 4)
        }
    
    # Common rejection reasons
    rejection_reasons = []
    for record in original_records:
        filter_result = record.get("sc_eqtl_filter_result", {})
        if not filter_result.get("passes_required_filters", True):
            rejection_reasons.extend(filter_result.get("filter_reasons", []))
    
    from collections import Counter
    reason_counts = Counter(rejection_reasons)
    report["common_rejection_reasons"] = [
        {"reason": reason, "count": count}
        for reason, count in reason_counts.most_common(10)
    ]
    
    return report

def build_geo_sra_mapping(
    geo_records: List[Dict[str, Any]],
    sra_records: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Build mapping between GEO and SRA records based on various criteria.
    
    Args:
        geo_records: List of GEO dataset records
        sra_records: List of SRA dataset records
        
    Returns:
        Mapping information and statistics
    """
    mapping = {
        "geo_to_sra": {},  # GEO accession -> list of SRA accessions
        "sra_to_geo": {},  # SRA accession -> GEO accession
        "orphaned_geo": [],  # GEO records without SRA matches
        "orphaned_sra": [],  # SRA records without GEO matches
        "mapping_stats": {},
        "mapping_methods": []
    }
    
    # Create lookup dictionaries
    geo_lookup = {record.get('geo_accession', ''): record for record in geo_records}
    sra_lookup = {record.get('run_accession', ''): record for record in sra_records}
    
    # Method 1: Direct GEO accession matching in SRA records
    for sra_record in sra_records:
        study_title = (sra_record.get('study_title') or '').upper()
        study_abstract = (sra_record.get('study_abstract') or '').upper()
        
        # Look for GEO accession patterns in SRA text
        import re
        geo_patterns = re.findall(r'GSE\d+', study_title + ' ' + study_abstract)
        
        for geo_acc in geo_patterns:
            if geo_acc in geo_lookup:
                # Found a match
                if geo_acc not in mapping["geo_to_sra"]:
                    mapping["geo_to_sra"][geo_acc] = []
                mapping["geo_to_sra"][geo_acc].append(sra_record.get('run_accession', ''))
                mapping["sra_to_geo"][sra_record.get('run_accession', '')] = geo_acc
    
    # Method 2: Title and organism similarity matching
    for geo_record in geo_records:
        geo_acc = geo_record.get('geo_accession', '')
        if geo_acc in mapping["geo_to_sra"]:
            continue  # Already matched
            
        geo_title = (geo_record.get('title') or '').lower()
        geo_organism = (geo_record.get('organism') or '').lower()
        
        # Find potential SRA matches
        potential_matches = []
        
        for sra_record in sra_records:
            sra_acc = sra_record.get('run_accession', '')
            if sra_acc in mapping["sra_to_geo"]:
                continue  # Already matched
                
            sra_title = (sra_record.get('study_title') or '').lower()
            sra_organism = (sra_record.get('organism') or '').lower()
            
            # Calculate similarity score
            similarity_score = calculate_record_similarity(geo_record, sra_record)
            
            if similarity_score > 0.7:  # High similarity threshold
                potential_matches.append((sra_acc, similarity_score))
        
        # Sort by similarity and take best matches
        potential_matches.sort(key=lambda x: x[1], reverse=True)
        
        if potential_matches:
            mapping["geo_to_sra"][geo_acc] = [match[0] for match in potential_matches[:5]]  # Top 5 matches
            for sra_acc, _ in potential_matches[:5]:
                mapping["sra_to_geo"][sra_acc] = geo_acc
    
    # Method 3: Sample count and date proximity matching
    for geo_record in geo_records:
        geo_acc = geo_record.get('geo_accession', '')
        if geo_acc in mapping["geo_to_sra"]:
            continue
            
        geo_sample_count = geo_record.get('sample_count', 0)
        geo_date = geo_record.get('submission_date', '') or ''
        geo_organism = geo_record.get('organism', '') or ''
        
        # Find SRA records with similar characteristics
        for sra_record in sra_records:
            sra_acc = sra_record.get('run_accession', '')
            if sra_acc in mapping["sra_to_geo"]:
                continue
                
            sra_organism = sra_record.get('organism', '') or ''
            sra_date = sra_record.get('submission_date', '') or ''
            
            # Check organism match
            if geo_organism.lower() != sra_organism.lower():
                continue
                
            # Check date proximity (within 6 months)
            if geo_date and sra_date:
                date_diff = calculate_date_difference(geo_date, sra_date)
                if date_diff <= 180:  # Within 6 months
                    if geo_acc not in mapping["geo_to_sra"]:
                        mapping["geo_to_sra"][geo_acc] = []
                    mapping["geo_to_sra"][geo_acc].append(sra_acc)
                    mapping["sra_to_geo"][sra_acc] = geo_acc
    
    # Identify orphaned records
    mapped_geo = set(mapping["geo_to_sra"].keys())
    mapped_sra = set(mapping["sra_to_geo"].keys())
    
    mapping["orphaned_geo"] = [
        record for record in geo_records 
        if record.get('geo_accession', '') not in mapped_geo
    ]
    
    mapping["orphaned_sra"] = [
        record for record in sra_records 
        if record.get('run_accession', '') not in mapped_sra
    ]
    
    # Generate mapping statistics
    mapping["mapping_stats"] = {
        "total_geo_records": len(geo_records),
        "total_sra_records": len(sra_records),
        "mapped_geo_records": len(mapped_geo),
        "mapped_sra_records": len(mapped_sra),
        "orphaned_geo_records": len(mapping["orphaned_geo"]),
        "orphaned_sra_records": len(mapping["orphaned_sra"]),
        "geo_mapping_rate": (len(mapped_geo) / len(geo_records)) * 100 if geo_records else 0,
        "sra_mapping_rate": (len(mapped_sra) / len(sra_records)) * 100 if sra_records else 0,
        "relationship_types": analyze_relationship_types(mapping["geo_to_sra"])
    }
    
    return mapping

def calculate_record_similarity(geo_record: Dict[str, Any], sra_record: Dict[str, Any]) -> float:
    """Calculate similarity score between GEO and SRA records."""
    score = 0.0
    
    # Title similarity
    geo_title = (geo_record.get('title') or '').lower()
    sra_title = (sra_record.get('study_title') or '').lower()
    
    if geo_title and sra_title:
        title_similarity = calculate_text_similarity(geo_title, sra_title)
        score += title_similarity * 0.4
    
    # Organism match
    geo_organism = (geo_record.get('organism') or '').lower()
    sra_organism = (sra_record.get('organism') or '').lower()
    
    if geo_organism == sra_organism:
        score += 0.3
    
    # Platform similarity
    geo_platform = (geo_record.get('platform') or '').lower()
    sra_platform = (sra_record.get('platform') or '').lower()
    
    if geo_platform and sra_platform:
        if geo_platform in sra_platform or sra_platform in geo_platform:
            score += 0.2
    
    # Sample count proximity
    # Safely convert sample count to integer
    try:
        geo_samples = geo_record.get('sample_count', 0)
        if isinstance(geo_samples, str):
            geo_samples = int(geo_samples) if geo_samples.isdigit() else 0
    except (ValueError, TypeError):
        geo_samples = 0
    
    # Safely convert spots to integer
    try:
        sra_spots = sra_record.get('spots', 0)
        if isinstance(sra_spots, str):
            sra_spots = int(sra_spots) if sra_spots.isdigit() else 0
        sra_samples = sra_spots // 1000000  # Rough conversion
    except (ValueError, TypeError):
        sra_samples = 0
    
    if geo_samples > 0 and sra_samples > 0:
        ratio = min(geo_samples, sra_samples) / max(geo_samples, sra_samples)
        score += ratio * 0.1
    
    return min(score, 1.0)

def calculate_text_similarity(text1: str, text2: str) -> float:
    """Calculate text similarity using simple word overlap."""
    if not text1 or not text2:
        return 0.0
    
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    # Remove common stop words
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}
    
    words1 = words1 - stop_words
    words2 = words2 - stop_words
    
    if not words1 or not words2:
        return 0.0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union)

def calculate_date_difference(date1: str, date2: str) -> int:
    """Calculate difference in days between two dates."""
    try:
        from datetime import datetime
        
        # Parse dates (assuming YYYY-MM-DD format)
        d1 = datetime.strptime(str(date1)[:10], '%Y-%m-%d')
        d2 = datetime.strptime(str(date2)[:10], '%Y-%m-%d')
        
        return abs((d1 - d2).days)
    except:
        return 999  # Large number if parsing fails

def analyze_relationship_types(geo_to_sra: Dict[str, List[str]]) -> Dict[str, int]:
    """Analyze the types of relationships between GEO and SRA."""
    relationship_types = {
        "one_to_one": 0,
        "one_to_many": 0,
        "many_to_one": 0,
        "many_to_many": 0
    }
    
    # Count GEO records by number of SRA matches
    for geo_acc, sra_list in geo_to_sra.items():
        if len(sra_list) == 1:
            relationship_types["one_to_one"] += 1
        else:
            relationship_types["one_to_many"] += 1
    
    return relationship_types

def generate_fastq_download_links(
    sra_records: List[Dict[str, Any]],
    base_url: str = "https://www.ncbi.nlm.nih.gov/sra"
) -> List[Dict[str, Any]]:
    """
    Generate FASTQ download links for SRA records.
    
    Args:
        sra_records: List of SRA records
        base_url: Base URL for SRA downloads
        
    Returns:
        List of records with download information
    """
    download_info = []
    
    for record in sra_records:
        run_accession = record.get('run_accession', '')
        study_accession = record.get('study_accession', '')
        
        if run_accession:
            # Generate various download options
            download_record = {
                "run_accession": run_accession,
                "study_accession": study_accession,
                "sra_url": f"{base_url}/{run_accession}",
                "fastq_dump_cmd": f"fastq-dump --split-files --gzip {run_accession}",
                "prefetch_cmd": f"prefetch {run_accession}",
                "fasterq_dump_cmd": f"fasterq-dump --split-files {run_accession}",
                "aspera_download": f"ascp -QT -l 300m -P33001 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/{run_accession[:6]}/{run_accession}/{run_accession}_1.fastq.gz ./",
                "estimated_size_gb": estimate_file_size(record),
                "library_layout": record.get('library_layout', 'SINGLE'),
                "library_strategy": record.get('library_strategy', ''),
                "platform": record.get('platform', ''),
                "instrument": record.get('instrument', ''),
                "spots": record.get('spots', 0),
                "bases": record.get('bases', 0)
            }
            
            download_info.append(download_record)
    
    return download_info

def estimate_file_size(sra_record: Dict[str, Any]) -> float:
    """Estimate FASTQ file size in GB based on SRA metadata."""
    bases = safe_int_convert(sra_record.get('bases', 0))
    
    if bases > 0:
        # Rough estimation: 1 base ≈ 1 byte in FASTQ format
        # Add overhead for quality scores and headers
        estimated_bytes = bases * 2  # Factor of 2 for quality scores
        estimated_gb = estimated_bytes / (1024 ** 3)
        return round(estimated_gb, 2)
    
    return 0.0

def create_integrated_dataset_table(
    geo_records: List[Dict[str, Any]],
    sra_records: List[Dict[str, Any]],
    mapping: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Create integrated dataset table with GEO-SRA relationships.
    
    Args:
        geo_records: List of GEO records
        sra_records: List of SRA records  
        mapping: GEO-SRA mapping information
        
    Returns:
        Integrated dataset table
    """
    integrated_table = []
    
    # Create lookup dictionaries
    geo_lookup = {record.get('geo_accession', ''): record for record in geo_records}
    sra_lookup = {record.get('run_accession', ''): record for record in sra_records}
    
    # Generate download links
    download_links = generate_fastq_download_links(sra_records)
    download_lookup = {link['run_accession']: link for link in download_links}
    
    # Process mapped GEO records
    for geo_acc, sra_list in mapping["geo_to_sra"].items():
        geo_record = geo_lookup.get(geo_acc, {})
        
        for sra_acc in sra_list:
            sra_record = sra_lookup.get(sra_acc, {})
            download_info = download_lookup.get(sra_acc, {})
            
            integrated_record = {
                # GEO Information
                "geo_accession": geo_acc,
                "geo_title": geo_record.get('title', ''),
                "geo_summary": geo_record.get('summary', ''),
                "geo_organism": geo_record.get('organism', ''),
                "geo_platform": geo_record.get('platform', ''),
                "geo_sample_count": safe_int_convert(geo_record.get('sample_count', 0)),
                "geo_submission_date": geo_record.get('submission_date', ''),
                "geo_status": geo_record.get('status', ''),
                
                # SRA Information
                "sra_run_accession": sra_acc,
                "sra_study_accession": sra_record.get('study_accession', ''),
                "sra_study_title": sra_record.get('study_title', ''),
                "sra_organism": sra_record.get('organism', ''),
                "sra_platform": sra_record.get('platform', ''),
                "sra_instrument": sra_record.get('instrument', ''),
                "sra_library_strategy": sra_record.get('library_strategy', ''),
                "sra_library_layout": sra_record.get('library_layout', ''),
                "sra_spots": safe_int_convert(sra_record.get('spots', 0)),
                "sra_bases": safe_int_convert(sra_record.get('bases', 0)),
                
                # Relationship Information
                "relationship_type": "geo_to_sra",
                "mapping_confidence": calculate_record_similarity(geo_record, sra_record),
                
                # Download Information
                "fastq_download_url": download_info.get('sra_url', ''),
                "fastq_dump_command": download_info.get('fastq_dump_cmd', ''),
                "prefetch_command": download_info.get('prefetch_cmd', ''),
                "fasterq_dump_command": download_info.get('fasterq_dump_cmd', ''),
                "aspera_download_command": download_info.get('aspera_download', ''),
                "estimated_file_size_gb": download_info.get('estimated_size_gb', 0),
                
                # Quality Information
                "data_completeness": calculate_data_completeness(geo_record, sra_record),
                "recommended_for_eqtl": assess_eqtl_recommendation(geo_record, sra_record)
            }
            
            integrated_table.append(integrated_record)
    
    # Add orphaned GEO records
    for geo_record in mapping["orphaned_geo"]:
        integrated_record = {
            # GEO Information
            "geo_accession": geo_record.get('geo_accession', ''),
            "geo_title": geo_record.get('title', ''),
            "geo_summary": geo_record.get('summary', ''),
            "geo_organism": geo_record.get('organism', ''),
            "geo_platform": geo_record.get('platform', ''),
            "geo_sample_count": safe_int_convert(geo_record.get('sample_count', 0)),
            "geo_submission_date": geo_record.get('submission_date', ''),
            "geo_status": geo_record.get('status', ''),
            
            # SRA Information (empty)
            "sra_run_accession": "",
            "sra_study_accession": "",
            "sra_study_title": "",
            "sra_organism": "",
            "sra_platform": "",
            "sra_instrument": "",
            "sra_library_strategy": "",
            "sra_library_layout": "",
            "sra_spots": 0,
            "sra_bases": 0,
            
            # Relationship Information
            "relationship_type": "geo_only",
            "mapping_confidence": 0.0,
            
            # Download Information (empty)
            "fastq_download_url": "",
            "fastq_dump_command": "",
            "prefetch_command": "",
            "fasterq_dump_command": "",
            "aspera_download_command": "",
            "estimated_file_size_gb": 0,
            
            # Quality Information
            "data_completeness": calculate_data_completeness(geo_record, {}),
            "recommended_for_eqtl": assess_eqtl_recommendation(geo_record, {})
        }
        
        integrated_table.append(integrated_record)
    
    # Add orphaned SRA records
    for sra_record in mapping["orphaned_sra"]:
        download_info = download_lookup.get(sra_record.get('run_accession', ''), {})
        
        integrated_record = {
            # GEO Information (empty)
            "geo_accession": "",
            "geo_title": "",
            "geo_summary": "",
            "geo_organism": "",
            "geo_platform": "",
            "geo_sample_count": 0,
            "geo_submission_date": "",
            "geo_status": "",
            
            # SRA Information
            "sra_run_accession": sra_record.get('run_accession', ''),
            "sra_study_accession": sra_record.get('study_accession', ''),
            "sra_study_title": sra_record.get('study_title', ''),
            "sra_organism": sra_record.get('organism', ''),
            "sra_platform": sra_record.get('platform', ''),
            "sra_instrument": sra_record.get('instrument', ''),
            "sra_library_strategy": sra_record.get('library_strategy', ''),
            "sra_library_layout": sra_record.get('library_layout', ''),
            "sra_spots": safe_int_convert(sra_record.get('spots', 0)),
            "sra_bases": safe_int_convert(sra_record.get('bases', 0)),
            
            # Relationship Information
            "relationship_type": "sra_only",
            "mapping_confidence": 0.0,
            
            # Download Information
            "fastq_download_url": download_info.get('sra_url', ''),
            "fastq_dump_command": download_info.get('fastq_dump_cmd', ''),
            "prefetch_command": download_info.get('prefetch_cmd', ''),
            "fasterq_dump_command": download_info.get('fasterq_dump_cmd', ''),
            "aspera_download_command": download_info.get('aspera_download', ''),
            "estimated_file_size_gb": download_info.get('estimated_size_gb', 0),
            
            # Quality Information
            "data_completeness": calculate_data_completeness({}, sra_record),
            "recommended_for_eqtl": assess_eqtl_recommendation({}, sra_record)
        }
        
        integrated_table.append(integrated_record)
    
    return integrated_table

def calculate_data_completeness(geo_record: Dict[str, Any], sra_record: Dict[str, Any]) -> float:
    """Calculate data completeness score."""
    score = 0.0
    total_fields = 10
    
    # Check key fields
    if geo_record.get('geo_accession'):
        score += 1
    if geo_record.get('title'):
        score += 1
    if geo_record.get('organism'):
        score += 1
    if geo_record.get('platform'):
        score += 1
    
    # Safely check sample count
    try:
        sample_count = geo_record.get('sample_count', 0)
        if isinstance(sample_count, str):
            sample_count = int(sample_count) if sample_count.isdigit() else 0
        if sample_count > 0:
            score += 1
    except (ValueError, TypeError):
        pass
    
    if sra_record.get('run_accession'):
        score += 1
    if sra_record.get('library_strategy'):
        score += 1
    if sra_record.get('platform'):
        score += 1
    
    # Safely check spots and bases
    try:
        spots = sra_record.get('spots', 0)
        if isinstance(spots, str):
            spots = int(spots) if spots.isdigit() else 0
        if spots > 0:
            score += 1
    except (ValueError, TypeError):
        pass
        
    try:
        bases = sra_record.get('bases', 0)
        if isinstance(bases, str):
            bases = int(bases) if bases.isdigit() else 0
        if bases > 0:
            score += 1
    except (ValueError, TypeError):
        pass
    
    return score / total_fields

def assess_eqtl_recommendation(geo_record: Dict[str, Any], sra_record: Dict[str, Any]) -> str:
    """Assess recommendation for eQTL analysis."""
    score = 0
    
    # Check organism
    organism = (geo_record.get('organism') or sra_record.get('organism') or '').lower()
    if 'homo sapiens' in organism:
        score += 2
    
    # Check single-cell indicators
    title = ((geo_record.get('title') or '') + ' ' + (sra_record.get('study_title') or '')).lower()
    if any(term in title for term in ['single cell', 'single-cell', 'scrna', 'sc-rna', '10x']):
        score += 2
    
    # Check sample size
    try:
        sample_count = geo_record.get('sample_count', 0)
        if isinstance(sample_count, str):
            sample_count = int(sample_count) if sample_count.isdigit() else 0
        if sample_count >= 100:
            score += 2
        elif sample_count >= 50:
            score += 1
    except (ValueError, TypeError):
        pass
    
    # Check data availability
    if geo_record.get('geo_accession') and sra_record.get('run_accession'):
        score += 1
    
    if score >= 5:
        return "Highly Recommended"
    elif score >= 3:
        return "Recommended"
    elif score >= 1:
        return "Consider"
    else:
        return "Not Recommended"

def apply_sc_eqtl_filters_with_ai(
    records: List[Dict[str, Any]],
    filter_config: Optional[Dict[str, Any]] = None,
    use_ai: bool = True,
    ai_batch_size: int = 10
) -> List[Dict[str, Any]]:
    """
    Apply comprehensive sc-eQTL filtering criteria with AI assistance.
    
    Args:
        records: List of dataset records
        filter_config: Configuration for filtering criteria
        use_ai: Whether to use AI for intelligent filtering
        ai_batch_size: Number of records to process with AI at once
        
    Returns:
        Filtered list of records with sc-eQTL suitability scores
    """
    if filter_config is None:
        # More lenient default configuration
        filter_config = {
            "required_species": ["Homo sapiens", "human"],
            "exclude_cell_lines": True,
            "require_database_id": True,
            "require_publication": False,
            "require_sample_size": False,
            "require_country_info": False,
            "require_age_info": False,
            "require_tumor_annotation": False,  # Made optional
            "require_sequencing_method": False,  # Made optional
            "require_tissue_source": False,     # Made optional
            "min_quality_score": 2,  # Lowered minimum score
            "ai_confidence_threshold": 0.5  # Lowered AI confidence threshold
        }
    
    logger.info(f"Starting AI-enhanced filtering for {len(records)} records")
    
    # Step 1: Apply basic keyword-based filtering (more lenient)
    basic_filtered = apply_basic_filters_lenient(records, filter_config)
    logger.info(f"Basic filtering retained {len(basic_filtered)} records")
    
    if not use_ai or not basic_filtered:
        return basic_filtered
    
    # Step 2: Apply AI-assisted filtering in batches
    ai_filtered = []
    
    try:
        from ..models.client import ModelClient
        model_client = ModelClient()
        
        for i in range(0, len(basic_filtered), ai_batch_size):
            batch = basic_filtered[i:i + ai_batch_size]
            logger.info(f"Processing AI batch {i//ai_batch_size + 1}/{(len(basic_filtered)-1)//ai_batch_size + 1}")
            
            try:
                ai_results = assess_records_with_ai(batch, model_client, filter_config)
                ai_filtered.extend(ai_results)
            except Exception as e:
                logger.warning(f"AI processing failed for batch {i//ai_batch_size + 1}: {e}")
                # Fall back to basic filtering for this batch
                ai_filtered.extend(batch)
    except ImportError:
        logger.warning("ModelClient not available, using basic filtering only")
        ai_filtered = basic_filtered
    
    logger.info(f"AI-enhanced filtering retained {len(ai_filtered)} records")
    return ai_filtered

def apply_basic_filters_lenient(
    records: List[Dict[str, Any]],
    filter_config: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Apply basic keyword-based filtering with more lenient criteria.
    """
    filtered_records = []
    
    for record in records:
        # Initialize filtering result
        filter_result = {
            "record": record,
            "passes_required_filters": True,
            "passes_optional_filters": True,
            "filter_scores": {},
            "filter_reasons": [],
            "overall_score": 0,
            "ai_assessed": False
        }
        
        # Apply more lenient basic filters
        filter_result["filter_scores"]["species"] = check_species_filter_lenient(record, filter_config["required_species"])
        filter_result["filter_scores"]["cell_line"] = check_cell_line_exclusion_lenient(record)
        filter_result["filter_scores"]["database_id"] = check_database_id_availability(record)
        filter_result["filter_scores"]["publication"] = check_publication_info_lenient(record)
        filter_result["filter_scores"]["sample_size"] = check_sample_size_info_lenient(record)
        filter_result["filter_scores"]["country"] = check_country_info_lenient(record)
        filter_result["filter_scores"]["age"] = check_age_info_lenient(record)
        filter_result["filter_scores"]["tumor"] = check_tumor_annotation_lenient(record)
        filter_result["filter_scores"]["sequencing_method"] = check_sequencing_method_lenient(record)
        filter_result["filter_scores"]["tissue"] = check_tissue_source_lenient(record)
        
        # Calculate overall score
        filter_result["overall_score"] = sum(filter_result["filter_scores"].values())
        
        # Apply minimum quality score threshold
        if filter_result["overall_score"] >= filter_config.get("min_quality_score", 2):
            # Check only critical required filters
            critical_failures = []
            
            # Database ID check (critical)
            if filter_result["filter_scores"]["database_id"] == 0:
                critical_failures.append("No valid database ID")
            
            if not critical_failures:
                filter_result["passes_required_filters"] = True
                filter_result["filter_reasons"] = []
            else:
                filter_result["passes_required_filters"] = False
                filter_result["filter_reasons"] = critical_failures
        else:
            filter_result["passes_required_filters"] = False
            filter_result["filter_reasons"] = ["Overall quality score too low"]
        
        # Add filter result to record
        record["sc_eqtl_filter_result"] = filter_result
        
        # Include records that pass basic filtering
        if filter_result["passes_required_filters"]:
            filtered_records.append(record)
    
    return filtered_records

def assess_records_with_ai(
    records: List[Dict[str, Any]],
    model_client,
    filter_config: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Use AI to assess records for sc-eQTL suitability.
    """
    # Prepare data for AI assessment
    records_for_ai = []
    for record in records:
        record_summary = {
            "accession": record.get("geo_accession") or record.get("run_accession", "Unknown"),
            "title": record.get("title", "")[:200],  # Truncate for efficiency
            "summary": record.get("summary", "")[:500],
            "organism": record.get("organism", ""),
            "tissue": record.get("tissue", ""),
            "platform": record.get("platform", ""),
            "sample_count": record.get("sample_count", 0),
            "library_strategy": record.get("library_strategy", ""),
            "basic_scores": record["sc_eqtl_filter_result"]["filter_scores"]
        }
        records_for_ai.append(record_summary)
    
    # Create AI prompt
    prompt = f"""
You are a professional single-cell genomics data analysis expert. Please evaluate whether the following datasets are suitable for sc-eQTL (single-cell expression quantitative trait loci) analysis.

Evaluation criteria:
1. Species: Prefer human (Homo sapiens)
2. Data type: Single-cell RNA sequencing data, avoid cell lines
3. Sample size: Multiple individuals' samples are better
4. Tissue source: Clear tissue or organ source
5. Technology platform: Modern single-cell sequencing technology
6. Data quality: Sufficient metadata description

Please evaluate the following {len(records_for_ai)} datasets and provide for each dataset:
- suitability_score: Suitability score from 0-10
- confidence: Confidence level from 0-1
- reasons: Evaluation reasons
- sc_eqtl_potential: Potential rating "High"/"Medium"/"Low"

Dataset information:
{records_for_ai}

Please return evaluation results in JSON format:
{{
    "assessments": [
        {{
            "accession": "dataset_id",
            "suitability_score": score,
            "confidence": confidence_level,
            "sc_eqtl_potential": "potential_level",
            "reasons": "evaluation_reasons",
            "recommended": true/false
        }}
    ]
}}
"""
    
    try:
        # Call AI model
        response = model_client.generate_response(prompt)
        
        # Try to parse JSON response
        import json
        try:
            ai_results = json.loads(response)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                ai_results = json.loads(json_match.group())
            else:
                raise ValueError("No valid JSON found in AI response")
        
        # Process AI results
        ai_filtered = []
        ai_assessments = ai_results.get("assessments", [])
        
        for i, record in enumerate(records):
            if i < len(ai_assessments):
                ai_assessment = ai_assessments[i]
                
                # Update record with AI assessment
                record["sc_eqtl_filter_result"]["ai_assessed"] = True
                record["sc_eqtl_filter_result"]["ai_suitability_score"] = ai_assessment.get("suitability_score", 0)
                record["sc_eqtl_filter_result"]["ai_confidence"] = ai_assessment.get("confidence", 0)
                record["sc_eqtl_filter_result"]["ai_potential"] = ai_assessment.get("sc_eqtl_potential", "Low")
                record["sc_eqtl_filter_result"]["ai_reasons"] = ai_assessment.get("reasons", "")
                record["sc_eqtl_filter_result"]["ai_recommended"] = ai_assessment.get("recommended", False)
                
                # Apply AI filtering threshold
                ai_confidence = ai_assessment.get("confidence", 0)
                ai_recommended = ai_assessment.get("recommended", False)
                
                if (ai_confidence >= filter_config.get("ai_confidence_threshold", 0.5) and 
                    ai_recommended):
                    ai_filtered.append(record)
                elif ai_confidence >= 0.3:  # Lower threshold for borderline cases
                    # Include borderline cases for manual review
                    record["sc_eqtl_filter_result"]["needs_manual_review"] = True
                    ai_filtered.append(record)
            else:
                # No AI assessment available, include based on basic filtering
                ai_filtered.append(record)
        
        return ai_filtered
        
    except Exception as e:
        logger.error(f"AI assessment failed: {e}")
        # Return original records if AI fails
        for record in records:
            record["sc_eqtl_filter_result"]["ai_assessed"] = False
            record["sc_eqtl_filter_result"]["ai_error"] = str(e)
        return records

# Lenient versions of filter functions
def check_species_filter_lenient(record: Dict[str, Any], required_species: List[str]) -> int:
    """More lenient species checking."""
    # Handle NULL values safely - use correct field names from database
    organism = (record.get('organism') or '').lower()
    title = (record.get('gse_title') or record.get('study_title') or '').lower()
    summary = (record.get('summary') or record.get('study_abstract') or '').lower()
    organism_ch1 = (record.get('organism_ch1') or '').lower()
    scientific_name = (record.get('scientific_name') or '').lower()
    common_name = (record.get('common_name') or '').lower()
    design_description = (record.get('design_description') or '').lower()
    
    # First, check explicit organism fields (highest priority)
    explicit_organism_fields = [organism, organism_ch1, scientific_name, common_name]
    for field in explicit_organism_fields:
        if field:  # Only check non-empty fields
            # Check for exact human species matches
            if 'homo sapiens' in field or field == 'human':
                return 2
            # Check for other required species
            for species in required_species:
                if species.lower() in field:
                    return 2
    
    # Second, check text fields for human indicators (medium priority)
    text_fields = [title, summary, design_description]
    text_to_check = ' '.join(text_fields)
    
    if text_to_check:
        # More specific human indicators to reduce false positives
        human_indicators = [
            'homo sapiens', 'human patient', 'human subject', 'human clinical',
            'human cancer', 'human tumor', 'human disease', 'human tissue',
            'human blood', 'human pbmc', 'human cell', 'human genome',
            'human breast', 'human lung', 'human liver', 'human brain'
        ]
        
        for identifier in human_indicators:
            if identifier in text_to_check:
                return 2
        
        # Check for required species in text
        for species in required_species:
            species_lower = species.lower()
            if species_lower in text_to_check:
                return 2
    
    # Third, check for broader human context indicators (lower priority)
    broader_indicators = ['patient', 'clinical', 'medical', 'hospital']
    broader_matches = 0
    for indicator in broader_indicators:
        if indicator in text_to_check:
            broader_matches += 1
    
    # If we have multiple broader indicators, it might be human-related
    if broader_matches >= 2:
        return 1  # Lower confidence human match
    
    return 0  # No required species found

def check_cell_line_exclusion_lenient(record: Dict[str, Any]) -> int:
    """More lenient cell line exclusion."""
    cell_line_keywords = [
        'hela', '293t', 'k562', 'jurkat', 'hek293', 'mcf7', 'a549',
        'cell line', 'cell-line', 'immortalized cell', 'transformed cell'
    ]
    
    # Handle NULL values safely - use correct field names from database
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(),
        (record.get('summary') or record.get('study_abstract') or '').lower(),
        (record.get('cell_type') or '').lower(),
        (record.get('source_name') or '').lower(),
        (record.get('source_name_ch1') or '').lower(),
        (record.get('characteristics_ch1') or '').lower(),
        (record.get('sample_name') or '').lower(),
        (record.get('gsm_title') or '').lower()
    ]
    
    for text in text_fields:
        for keyword in cell_line_keywords:
            if keyword in text:
                # Additional check for primary vs cell line
                if 'primary' in text or 'fresh' in text or 'tissue' in text:
                    return 1  # Possibly mixed, not excluded
                return 0  # Likely cell line
    
    return 2  # No cell line detected

def check_database_id_availability(record: Dict[str, Any]) -> int:
    """Check if record has valid database IDs."""
    # Check GEO accession - use correct field name 'gse'
    geo_accession = (record.get('gse') or '').strip()
    
    # Check SRA accession - use correct field names
    sra_accession = (record.get('run_accession') or record.get('study_accession') or '').strip()
    
    if geo_accession and geo_accession.startswith('GSE'):
        return 2  # Valid GEO ID
    elif sra_accession and (sra_accession.startswith('SRR') or sra_accession.startswith('SRP') or 
                           sra_accession.startswith('ERR') or sra_accession.startswith('DRR')):
        return 2  # Valid SRA ID (including European and Japanese archives)
    
    return 0  # No valid database ID

def check_publication_info(record: Dict[str, Any]) -> int:
    """Check if record has publication information."""
    pmid = record.get('pmid', '')
    doi = record.get('doi', '')
    pubmed_id = record.get('pubmed_id', '')
    
    if pmid or doi or pubmed_id:
        return 1  # Has publication info
    
    # Check in title/summary for publication keywords
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower()
    ]
    
    pub_keywords = ['pmid', 'doi', 'pubmed', 'published', 'paper', 'article']
    
    for text in text_fields:
        for keyword in pub_keywords:
            if keyword in text:
                return 1  # Likely has publication info
    
    return 0  # No publication info

def check_sample_size_info(record: Dict[str, Any]) -> int:
    """Check if record has sample size information."""
    sample_count = safe_int_convert(record.get('sample_count', 0))
    
    if sample_count and sample_count > 0:
        if sample_count >= 100:
            return 2  # Large sample size
        elif sample_count >= 20:
            return 1  # Adequate sample size
        else:
            return 0  # Small sample size
    
    # Check in text for sample size mentions
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower()
    ]
    
    import re
    for text in text_fields:
        # Look for patterns like "n=50", "50 samples", "50 subjects"
        matches = re.findall(r'(?:n\s*=\s*|samples?\s*=\s*|subjects?\s*=\s*|patients?\s*=\s*)?(\d+)\s*(?:samples?|subjects?|patients?|individuals?)', text)
        if matches:
            max_size = max(int(match) for match in matches)
            if max_size >= 100:
                return 2
            elif max_size >= 20:
                return 1
    
    return 0  # No sample size info

def check_country_info(record: Dict[str, Any]) -> int:
    """Check if record has country/geographic information."""
    country_keywords = [
        'usa', 'united states', 'china', 'uk', 'united kingdom', 'germany', 
        'japan', 'france', 'canada', 'australia', 'sweden', 'netherlands',
        'country', 'nation', 'geographic', 'location', 'region'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('country', '').lower(),
        record.get('geographic_location', '').lower()
    ]
    
    for text in text_fields:
        for keyword in country_keywords:
            if keyword in text:
                return 1  # Has country info
    
    return 0  # No country info

def check_age_info(record: Dict[str, Any]) -> int:
    """Check if record has age information."""
    age_keywords = [
        'age', 'years old', 'year-old', 'adult', 'pediatric', 'elderly',
        'infant', 'child', 'adolescent', 'birth', 'born'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('age', '').lower(),
        record.get('characteristics', '').lower()
    ]
    
    import re
    for text in text_fields:
        # Look for age patterns
        age_patterns = [
            r'\b\d+\s*(?:years?\s*old|y\.?o\.?|yr\.?s?)\b',
            r'\b(?:age|aged)\s*\d+',
            r'\b\d+\s*-\s*\d+\s*(?:years?|y\.?o\.?)'
        ]
        
        for pattern in age_patterns:
            if re.search(pattern, text):
                return 1  # Has age info
        
        for keyword in age_keywords:
            if keyword in text:
                return 1  # Has age-related info
    
    return 0  # No age info

def check_tumor_annotation(record: Dict[str, Any]) -> int:
    """Check if record has tumor status annotation."""
    tumor_keywords = [
        'tumor', 'tumour', 'cancer', 'carcinoma', 'adenocarcinoma', 'malignant',
        'normal', 'healthy', 'control', 'non-tumor', 'benign'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('tissue', '').lower(),
        record.get('disease', '').lower(),
        record.get('characteristics', '').lower()
    ]
    
    for text in text_fields:
        for keyword in tumor_keywords:
            if keyword in text:
                return 1  # Has tumor status annotation
    
    return 0  # No tumor status annotation

def check_sequencing_method(record: Dict[str, Any]) -> int:
    """Check if record has clear sequencing method."""
    sequencing_methods = [
        '10x genomics', '10x', 'smart-seq', 'smart-seq2', 'drop-seq',
        'cel-seq', 'mars-seq', 'scrb-seq', 'plate-seq', 'well-seq',
        'single-cell', 'single cell', 'sc-rna', 'scrna'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('library_strategy', '').lower(),
        record.get('platform', '').lower(),
        record.get('instrument', '').lower()
    ]
    
    for text in text_fields:
        for method in sequencing_methods:
            if method in text:
                return 2  # Clear sequencing method
    
    return 0  # No clear sequencing method

def check_tissue_source(record: Dict[str, Any]) -> int:
    """Check if record has clear tissue source."""
    tissue_keywords = [
        'brain', 'liver', 'heart', 'lung', 'kidney', 'muscle', 'blood',
        'skin', 'bone', 'pancreas', 'stomach', 'intestine', 'colon',
        'breast', 'ovary', 'testis', 'prostate', 'thyroid', 'spleen',
        'tissue', 'organ', 'biopsy', 'sample'
    ]
    
    text_fields = [
        record.get('title', '').lower(),
        record.get('summary', '').lower(),
        record.get('tissue', '').lower(),
        record.get('organ', '').lower(),
        record.get('source_name', '').lower()
    ]
    
    for text in text_fields:
        for keyword in tissue_keywords:
            if keyword in text:
                return 1  # Has tissue source
    
    return 0  # No clear tissue source

def generate_filter_report(
    original_records: List[Dict[str, Any]],
    filtered_records: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """Generate comprehensive filtering report."""
    
    report = {
        "filtering_summary": {
            "original_count": len(original_records),
            "filtered_count": len(filtered_records),
            "retention_rate": (len(filtered_records) / len(original_records)) * 100 if original_records else 0
        },
        "filter_statistics": {},
        "quality_distribution": {},
        "common_rejection_reasons": [],
        "timestamp": datetime.now().isoformat()
    }
    
    # Analyze filter statistics
    filter_categories = [
        "species", "cell_line", "database_id", "publication", "sample_size",
        "country", "age", "tumor", "sequencing_method", "tissue"
    ]
    
    for category in filter_categories:
        scores = []
        for record in original_records:
            filter_result = record.get("sc_eqtl_filter_result", {})
            score = filter_result.get("filter_scores", {}).get(category, 0)
            scores.append(score)
        
        if scores:
            report["filter_statistics"][category] = {
                "average_score": sum(scores) / len(scores),
                "pass_count": sum(1 for s in scores if s > 0),
                "fail_count": sum(1 for s in scores if s == 0),
                "pass_rate": (sum(1 for s in scores if s > 0) / len(scores)) * 100
            }
    
    # Analyze quality distribution
    quality_scores = []
    for record in filtered_records:
        filter_result = record.get("sc_eqtl_filter_result", {})
        score = filter_result.get("overall_score", 0)
        quality_scores.append(score)
    
    if quality_scores:
        report["quality_distribution"] = {
            "average_score": sum(quality_scores) / len(quality_scores),
            "min_score": min(quality_scores),
            "max_score": max(quality_scores),
            "high_quality_count": sum(1 for s in quality_scores if s >= 8),
            "medium_quality_count": sum(1 for s in quality_scores if 4 <= s < 8),
            "low_quality_count": sum(1 for s in quality_scores if s < 4)
        }
    
    # Common rejection reasons
    rejection_reasons = []
    for record in original_records:
        filter_result = record.get("sc_eqtl_filter_result", {})
        if not filter_result.get("passes_required_filters", True):
            rejection_reasons.extend(filter_result.get("filter_reasons", []))
    
    from collections import Counter
    reason_counts = Counter(rejection_reasons)
    report["common_rejection_reasons"] = [
        {"reason": reason, "count": count}
        for reason, count in reason_counts.most_common(10)
    ]
    
    return report

def build_geo_sra_mapping(
    geo_records: List[Dict[str, Any]],
    sra_records: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Build mapping between GEO and SRA records based on various criteria.
    
    Args:
        geo_records: List of GEO dataset records
        sra_records: List of SRA dataset records
        
    Returns:
        Mapping information and statistics
    """
    mapping = {
        "geo_to_sra": {},  # GEO accession -> list of SRA accessions
        "sra_to_geo": {},  # SRA accession -> GEO accession
        "orphaned_geo": [],  # GEO records without SRA matches
        "orphaned_sra": [],  # SRA records without GEO matches
        "mapping_stats": {},
        "mapping_methods": []
    }
    
    # Create lookup dictionaries
    geo_lookup = {record.get('geo_accession', ''): record for record in geo_records}
    sra_lookup = {record.get('run_accession', ''): record for record in sra_records}
    
    # Method 1: Direct GEO accession matching in SRA records
    for sra_record in sra_records:
        study_title = (sra_record.get('study_title') or '').upper()
        study_abstract = (sra_record.get('study_abstract') or '').upper()
        
        # Look for GEO accession patterns in SRA text
        import re
        geo_patterns = re.findall(r'GSE\d+', study_title + ' ' + study_abstract)
        
        for geo_acc in geo_patterns:
            if geo_acc in geo_lookup:
                # Found a match
                if geo_acc not in mapping["geo_to_sra"]:
                    mapping["geo_to_sra"][geo_acc] = []
                mapping["geo_to_sra"][geo_acc].append(sra_record.get('run_accession', ''))
                mapping["sra_to_geo"][sra_record.get('run_accession', '')] = geo_acc
    
    # Method 2: Title and organism similarity matching
    for geo_record in geo_records:
        geo_acc = geo_record.get('geo_accession', '')
        if geo_acc in mapping["geo_to_sra"]:
            continue  # Already matched
            
        geo_title = (geo_record.get('title') or '').lower()
        geo_organism = (geo_record.get('organism') or '').lower()
        
        # Find potential SRA matches
        potential_matches = []
        
        for sra_record in sra_records:
            sra_acc = sra_record.get('run_accession', '')
            if sra_acc in mapping["sra_to_geo"]:
                continue  # Already matched
                
            sra_title = (sra_record.get('study_title') or '').lower()
            sra_organism = (sra_record.get('organism') or '').lower()
            
            # Calculate similarity score
            similarity_score = calculate_record_similarity(geo_record, sra_record)
            
            if similarity_score > 0.7:  # High similarity threshold
                potential_matches.append((sra_acc, similarity_score))
        
        # Sort by similarity and take best matches
        potential_matches.sort(key=lambda x: x[1], reverse=True)
        
        if potential_matches:
            mapping["geo_to_sra"][geo_acc] = [match[0] for match in potential_matches[:5]]  # Top 5 matches
            for sra_acc, _ in potential_matches[:5]:
                mapping["sra_to_geo"][sra_acc] = geo_acc
    
    # Method 3: Sample count and date proximity matching
    for geo_record in geo_records:
        geo_acc = geo_record.get('geo_accession', '')
        if geo_acc in mapping["geo_to_sra"]:
            continue
            
        geo_sample_count = geo_record.get('sample_count', 0)
        geo_date = geo_record.get('submission_date', '') or ''
        geo_organism = geo_record.get('organism', '') or ''
        
        # Find SRA records with similar characteristics
        for sra_record in sra_records:
            sra_acc = sra_record.get('run_accession', '')
            if sra_acc in mapping["sra_to_geo"]:
                continue
                
            sra_organism = sra_record.get('organism', '') or ''
            sra_date = sra_record.get('submission_date', '') or ''
            
            # Check organism match
            if geo_organism.lower() != sra_organism.lower():
                continue
                
            # Check date proximity (within 6 months)
            if geo_date and sra_date:
                date_diff = calculate_date_difference(geo_date, sra_date)
                if date_diff <= 180:  # Within 6 months
                    if geo_acc not in mapping["geo_to_sra"]:
                        mapping["geo_to_sra"][geo_acc] = []
                    mapping["geo_to_sra"][geo_acc].append(sra_acc)
                    mapping["sra_to_geo"][sra_acc] = geo_acc
    
    # Identify orphaned records
    mapped_geo = set(mapping["geo_to_sra"].keys())
    mapped_sra = set(mapping["sra_to_geo"].keys())
    
    mapping["orphaned_geo"] = [
        record for record in geo_records 
        if record.get('geo_accession', '') not in mapped_geo
    ]
    
    mapping["orphaned_sra"] = [
        record for record in sra_records 
        if record.get('run_accession', '') not in mapped_sra
    ]
    
    # Generate mapping statistics
    mapping["mapping_stats"] = {
        "total_geo_records": len(geo_records),
        "total_sra_records": len(sra_records),
        "mapped_geo_records": len(mapped_geo),
        "mapped_sra_records": len(mapped_sra),
        "orphaned_geo_records": len(mapping["orphaned_geo"]),
        "orphaned_sra_records": len(mapping["orphaned_sra"]),
        "geo_mapping_rate": (len(mapped_geo) / len(geo_records)) * 100 if geo_records else 0,
        "sra_mapping_rate": (len(mapped_sra) / len(sra_records)) * 100 if sra_records else 0,
        "relationship_types": analyze_relationship_types(mapping["geo_to_sra"])
    }
    
    return mapping

def calculate_record_similarity(geo_record: Dict[str, Any], sra_record: Dict[str, Any]) -> float:
    """Calculate similarity score between GEO and SRA records."""
    score = 0.0
    
    # Title similarity
    geo_title = (geo_record.get('title') or '').lower()
    sra_title = (sra_record.get('study_title') or '').lower()
    
    if geo_title and sra_title:
        title_similarity = calculate_text_similarity(geo_title, sra_title)
        score += title_similarity * 0.4
    
    # Organism match
    geo_organism = (geo_record.get('organism') or '').lower()
    sra_organism = (sra_record.get('organism') or '').lower()
    
    if geo_organism == sra_organism:
        score += 0.3
    
    # Platform similarity
    geo_platform = (geo_record.get('platform') or '').lower()
    sra_platform = (sra_record.get('platform') or '').lower()
    
    if geo_platform and sra_platform:
        if geo_platform in sra_platform or sra_platform in geo_platform:
            score += 0.2
    
    # Sample count proximity
    # Safely convert sample count to integer
    try:
        geo_samples = geo_record.get('sample_count', 0)
        if isinstance(geo_samples, str):
            geo_samples = int(geo_samples) if geo_samples.isdigit() else 0
    except (ValueError, TypeError):
        geo_samples = 0
    
    # Safely convert spots to integer
    try:
        sra_spots = sra_record.get('spots', 0)
        if isinstance(sra_spots, str):
            sra_spots = int(sra_spots) if sra_spots.isdigit() else 0
        sra_samples = sra_spots // 1000000  # Rough conversion
    except (ValueError, TypeError):
        sra_samples = 0
    
    if geo_samples > 0 and sra_samples > 0:
        ratio = min(geo_samples, sra_samples) / max(geo_samples, sra_samples)
        score += ratio * 0.1
    
    return min(score, 1.0)

def calculate_text_similarity(text1: str, text2: str) -> float:
    """Calculate text similarity using simple word overlap."""
    if not text1 or not text2:
        return 0.0
    
    words1 = set(text1.lower().split())
    words2 = set(text2.lower().split())
    
    # Remove common stop words
    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'}
    
    words1 = words1 - stop_words
    words2 = words2 - stop_words
    
    if not words1 or not words2:
        return 0.0
    
    intersection = words1.intersection(words2)
    union = words1.union(words2)
    
    return len(intersection) / len(union)

def calculate_date_difference(date1: str, date2: str) -> int:
    """Calculate difference in days between two dates."""
    try:
        from datetime import datetime
        
        # Parse dates (assuming YYYY-MM-DD format)
        d1 = datetime.strptime(str(date1)[:10], '%Y-%m-%d')
        d2 = datetime.strptime(str(date2)[:10], '%Y-%m-%d')
        
        return abs((d1 - d2).days)
    except:
        return 999  # Large number if parsing fails

def analyze_relationship_types(geo_to_sra: Dict[str, List[str]]) -> Dict[str, int]:
    """Analyze the types of relationships between GEO and SRA."""
    relationship_types = {
        "one_to_one": 0,
        "one_to_many": 0,
        "many_to_one": 0,
        "many_to_many": 0
    }
    
    # Count GEO records by number of SRA matches
    for geo_acc, sra_list in geo_to_sra.items():
        if len(sra_list) == 1:
            relationship_types["one_to_one"] += 1
        else:
            relationship_types["one_to_many"] += 1
    
    return relationship_types

def generate_fastq_download_links(
    sra_records: List[Dict[str, Any]],
    base_url: str = "https://www.ncbi.nlm.nih.gov/sra"
) -> List[Dict[str, Any]]:
    """
    Generate FASTQ download links for SRA records.
    
    Args:
        sra_records: List of SRA records
        base_url: Base URL for SRA downloads
        
    Returns:
        List of records with download information
    """
    download_info = []
    
    for record in sra_records:
        run_accession = record.get('run_accession', '')
        study_accession = record.get('study_accession', '')
        
        if run_accession:
            # Generate various download options
            download_record = {
                "run_accession": run_accession,
                "study_accession": study_accession,
                "sra_url": f"{base_url}/{run_accession}",
                "fastq_dump_cmd": f"fastq-dump --split-files --gzip {run_accession}",
                "prefetch_cmd": f"prefetch {run_accession}",
                "fasterq_dump_cmd": f"fasterq-dump --split-files {run_accession}",
                "aspera_download": f"ascp -QT -l 300m -P33001 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh era-fasp@fasp.sra.ebi.ac.uk:/vol1/fastq/{run_accession[:6]}/{run_accession}/{run_accession}_1.fastq.gz ./",
                "estimated_size_gb": estimate_file_size(record),
                "library_layout": record.get('library_layout', 'SINGLE'),
                "library_strategy": record.get('library_strategy', ''),
                "platform": record.get('platform', ''),
                "instrument": record.get('instrument', ''),
                "spots": record.get('spots', 0),
                "bases": record.get('bases', 0)
            }
            
            download_info.append(download_record)
    
    return download_info

def estimate_file_size(sra_record: Dict[str, Any]) -> float:
    """Estimate FASTQ file size in GB based on SRA metadata."""
    bases = safe_int_convert(sra_record.get('bases', 0))
    
    if bases > 0:
        # Rough estimation: 1 base ≈ 1 byte in FASTQ format
        # Add overhead for quality scores and headers
        estimated_bytes = bases * 2  # Factor of 2 for quality scores
        estimated_gb = estimated_bytes / (1024 ** 3)
        return round(estimated_gb, 2)
    
    return 0.0

def create_integrated_dataset_table(
    geo_records: List[Dict[str, Any]],
    sra_records: List[Dict[str, Any]],
    mapping: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Create integrated dataset table with GEO-SRA relationships.
    
    Args:
        geo_records: List of GEO records
        sra_records: List of SRA records  
        mapping: GEO-SRA mapping information
        
    Returns:
        Integrated dataset table
    """
    integrated_table = []
    
    # Create lookup dictionaries
    geo_lookup = {record.get('geo_accession', ''): record for record in geo_records}
    sra_lookup = {record.get('run_accession', ''): record for record in sra_records}
    
    # Generate download links
    download_links = generate_fastq_download_links(sra_records)
    download_lookup = {link['run_accession']: link for link in download_links}
    
    # Process mapped GEO records
    for geo_acc, sra_list in mapping["geo_to_sra"].items():
        geo_record = geo_lookup.get(geo_acc, {})
        
        for sra_acc in sra_list:
            sra_record = sra_lookup.get(sra_acc, {})
            download_info = download_lookup.get(sra_acc, {})
            
            integrated_record = {
                # GEO Information
                "geo_accession": geo_acc,
                "geo_title": geo_record.get('title', ''),
                "geo_summary": geo_record.get('summary', ''),
                "geo_organism": geo_record.get('organism', ''),
                "geo_platform": geo_record.get('platform', ''),
                "geo_sample_count": safe_int_convert(geo_record.get('sample_count', 0)),
                "geo_submission_date": geo_record.get('submission_date', ''),
                "geo_status": geo_record.get('status', ''),
                
                # SRA Information
                "sra_run_accession": sra_acc,
                "sra_study_accession": sra_record.get('study_accession', ''),
                "sra_study_title": sra_record.get('study_title', ''),
                "sra_organism": sra_record.get('organism', ''),
                "sra_platform": sra_record.get('platform', ''),
                "sra_instrument": sra_record.get('instrument', ''),
                "sra_library_strategy": sra_record.get('library_strategy', ''),
                "sra_library_layout": sra_record.get('library_layout', ''),
                "sra_spots": safe_int_convert(sra_record.get('spots', 0)),
                "sra_bases": safe_int_convert(sra_record.get('bases', 0)),
                
                # Relationship Information
                "relationship_type": "geo_to_sra",
                "mapping_confidence": calculate_record_similarity(geo_record, sra_record),
                
                # Download Information
                "fastq_download_url": download_info.get('sra_url', ''),
                "fastq_dump_command": download_info.get('fastq_dump_cmd', ''),
                "prefetch_command": download_info.get('prefetch_cmd', ''),
                "fasterq_dump_command": download_info.get('fasterq_dump_cmd', ''),
                "aspera_download_command": download_info.get('aspera_download', ''),
                "estimated_file_size_gb": download_info.get('estimated_size_gb', 0),
                
                # Quality Information
                "data_completeness": calculate_data_completeness(geo_record, sra_record),
                "recommended_for_eqtl": assess_eqtl_recommendation(geo_record, sra_record)
            }
            
            integrated_table.append(integrated_record)
    
    # Add orphaned GEO records
    for geo_record in mapping["orphaned_geo"]:
        integrated_record = {
            # GEO Information
            "geo_accession": geo_record.get('geo_accession', ''),
            "geo_title": geo_record.get('title', ''),
            "geo_summary": geo_record.get('summary', ''),
            "geo_organism": geo_record.get('organism', ''),
            "geo_platform": geo_record.get('platform', ''),
            "geo_sample_count": safe_int_convert(geo_record.get('sample_count', 0)),
            "geo_submission_date": geo_record.get('submission_date', ''),
            "geo_status": geo_record.get('status', ''),
            
            # SRA Information (empty)
            "sra_run_accession": "",
            "sra_study_accession": "",
            "sra_study_title": "",
            "sra_organism": "",
            "sra_platform": "",
            "sra_instrument": "",
            "sra_library_strategy": "",
            "sra_library_layout": "",
            "sra_spots": 0,
            "sra_bases": 0,
            
            # Relationship Information
            "relationship_type": "geo_only",
            "mapping_confidence": 0.0,
            
            # Download Information (empty)
            "fastq_download_url": "",
            "fastq_dump_command": "",
            "prefetch_command": "",
            "fasterq_dump_command": "",
            "aspera_download_command": "",
            "estimated_file_size_gb": 0,
            
            # Quality Information
            "data_completeness": calculate_data_completeness(geo_record, {}),
            "recommended_for_eqtl": assess_eqtl_recommendation(geo_record, {})
        }
        
        integrated_table.append(integrated_record)
    
    # Add orphaned SRA records
    for sra_record in mapping["orphaned_sra"]:
        download_info = download_lookup.get(sra_record.get('run_accession', ''), {})
        
        integrated_record = {
            # GEO Information (empty)
            "geo_accession": "",
            "geo_title": "",
            "geo_summary": "",
            "geo_organism": "",
            "geo_platform": "",
            "geo_sample_count": 0,
            "geo_submission_date": "",
            "geo_status": "",
            
            # SRA Information
            "sra_run_accession": sra_record.get('run_accession', ''),
            "sra_study_accession": sra_record.get('study_accession', ''),
            "sra_study_title": sra_record.get('study_title', ''),
            "sra_organism": sra_record.get('organism', ''),
            "sra_platform": sra_record.get('platform', ''),
            "sra_instrument": sra_record.get('instrument', ''),
            "sra_library_strategy": sra_record.get('library_strategy', ''),
            "sra_library_layout": sra_record.get('library_layout', ''),
            "sra_spots": safe_int_convert(sra_record.get('spots', 0)),
            "sra_bases": safe_int_convert(sra_record.get('bases', 0)),
            
            # Relationship Information
            "relationship_type": "sra_only",
            "mapping_confidence": 0.0,
            
            # Download Information
            "fastq_download_url": download_info.get('sra_url', ''),
            "fastq_dump_command": download_info.get('fastq_dump_cmd', ''),
            "prefetch_command": download_info.get('prefetch_cmd', ''),
            "fasterq_dump_command": download_info.get('fasterq_dump_cmd', ''),
            "aspera_download_command": download_info.get('aspera_download', ''),
            "estimated_file_size_gb": download_info.get('estimated_size_gb', 0),
            
            # Quality Information
            "data_completeness": calculate_data_completeness({}, sra_record),
            "recommended_for_eqtl": assess_eqtl_recommendation({}, sra_record)
        }
        
        integrated_table.append(integrated_record)
    
    return integrated_table

def calculate_data_completeness(geo_record: Dict[str, Any], sra_record: Dict[str, Any]) -> float:
    """Calculate data completeness score."""
    score = 0.0
    total_fields = 10
    
    # Check key fields
    if geo_record.get('geo_accession'):
        score += 1
    if geo_record.get('title'):
        score += 1
    if geo_record.get('organism'):
        score += 1
    if geo_record.get('platform'):
        score += 1
    
    # Safely check sample count
    try:
        sample_count = geo_record.get('sample_count', 0)
        if isinstance(sample_count, str):
            sample_count = int(sample_count) if sample_count.isdigit() else 0
        if sample_count > 0:
            score += 1
    except (ValueError, TypeError):
        pass
    
    if sra_record.get('run_accession'):
        score += 1
    if sra_record.get('library_strategy'):
        score += 1
    if sra_record.get('platform'):
        score += 1
    
    # Safely check spots and bases
    try:
        spots = sra_record.get('spots', 0)
        if isinstance(spots, str):
            spots = int(spots) if spots.isdigit() else 0
        if spots > 0:
            score += 1
    except (ValueError, TypeError):
        pass
        
    try:
        bases = sra_record.get('bases', 0)
        if isinstance(bases, str):
            bases = int(bases) if bases.isdigit() else 0
        if bases > 0:
            score += 1
    except (ValueError, TypeError):
        pass
    
    return score / total_fields

def assess_eqtl_recommendation(geo_record: Dict[str, Any], sra_record: Dict[str, Any]) -> str:
    """Assess recommendation for eQTL analysis."""
    score = 0
    
    # Check organism
    organism = (geo_record.get('organism') or sra_record.get('organism') or '').lower()
    if 'homo sapiens' in organism:
        score += 2
    
    # Check single-cell indicators
    title = ((geo_record.get('title') or '') + ' ' + (sra_record.get('study_title') or '')).lower()
    if any(term in title for term in ['single cell', 'single-cell', 'scrna', 'sc-rna', '10x']):
        score += 2
    
    # Check sample size
    try:
        sample_count = geo_record.get('sample_count', 0)
        if isinstance(sample_count, str):
            sample_count = int(sample_count) if sample_count.isdigit() else 0
        if sample_count >= 100:
            score += 2
        elif sample_count >= 50:
            score += 1
    except (ValueError, TypeError):
        pass
    
    # Check data availability
    if geo_record.get('geo_accession') and sra_record.get('run_accession'):
        score += 1
    
    if score >= 5:
        return "Highly Recommended"
    elif score >= 3:
        return "Recommended"
    elif score >= 1:
        return "Consider"
    else:
        return "Not Recommended"

def apply_sc_eqtl_filters_with_ai(
    records: List[Dict[str, Any]],
    filter_config: Optional[Dict[str, Any]] = None,
    use_ai: bool = True,
    ai_batch_size: int = 10
) -> List[Dict[str, Any]]:
    """
    Apply comprehensive sc-eQTL filtering criteria with AI assistance.
    
    Args:
        records: List of dataset records
        filter_config: Configuration for filtering criteria
        use_ai: Whether to use AI for intelligent filtering
        ai_batch_size: Number of records to process with AI at once
        
    Returns:
        Filtered list of records with sc-eQTL suitability scores
    """
    if filter_config is None:
        # More lenient default configuration
        filter_config = {
            "required_species": ["Homo sapiens", "human"],
            "exclude_cell_lines": True,
            "require_database_id": True,
            "require_publication": False,
            "require_sample_size": False,
            "require_country_info": False,
            "require_age_info": False,
            "require_tumor_annotation": False,  # Made optional
            "require_sequencing_method": False,  # Made optional
            "require_tissue_source": False,     # Made optional
            "min_quality_score": 2,  # Lowered minimum score
            "ai_confidence_threshold": 0.5  # Lowered AI confidence threshold
        }
    
    logger.info(f"Starting AI-enhanced filtering for {len(records)} records")
    
    # Step 1: Apply basic keyword-based filtering (more lenient)
    basic_filtered = apply_basic_filters_lenient(records, filter_config)
    logger.info(f"Basic filtering retained {len(basic_filtered)} records")
    
    if not use_ai or not basic_filtered:
        return basic_filtered
    
    # Step 2: Apply AI-assisted filtering in batches
    ai_filtered = []
    
    try:
        from ..models.client import ModelClient
        model_client = ModelClient()
        
        for i in range(0, len(basic_filtered), ai_batch_size):
            batch = basic_filtered[i:i + ai_batch_size]
            logger.info(f"Processing AI batch {i//ai_batch_size + 1}/{(len(basic_filtered)-1)//ai_batch_size + 1}")
            
            try:
                ai_results = assess_records_with_ai(batch, model_client, filter_config)
                ai_filtered.extend(ai_results)
            except Exception as e:
                logger.warning(f"AI processing failed for batch {i//ai_batch_size + 1}: {e}")
                # Fall back to basic filtering for this batch
                ai_filtered.extend(batch)
    except ImportError:
        logger.warning("ModelClient not available, using basic filtering only")
        ai_filtered = basic_filtered
    
    logger.info(f"AI-enhanced filtering retained {len(ai_filtered)} records")
    return ai_filtered

def apply_basic_filters_lenient(
    records: List[Dict[str, Any]],
    filter_config: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Apply basic keyword-based filtering with more lenient criteria.
    """
    filtered_records = []
    
    for record in records:
        # Initialize filtering result
        filter_result = {
            "record": record,
            "passes_required_filters": True,
            "passes_optional_filters": True,
            "filter_scores": {},
            "filter_reasons": [],
            "overall_score": 0,
            "ai_assessed": False
        }
        
        # Apply more lenient basic filters
        filter_result["filter_scores"]["species"] = check_species_filter_lenient(record, filter_config["required_species"])
        filter_result["filter_scores"]["cell_line"] = check_cell_line_exclusion_lenient(record)
        filter_result["filter_scores"]["database_id"] = check_database_id_availability(record)
        filter_result["filter_scores"]["publication"] = check_publication_info_lenient(record)
        filter_result["filter_scores"]["sample_size"] = check_sample_size_info_lenient(record)
        filter_result["filter_scores"]["country"] = check_country_info_lenient(record)
        filter_result["filter_scores"]["age"] = check_age_info_lenient(record)
        filter_result["filter_scores"]["tumor"] = check_tumor_annotation_lenient(record)
        filter_result["filter_scores"]["sequencing_method"] = check_sequencing_method_lenient(record)
        filter_result["filter_scores"]["tissue"] = check_tissue_source_lenient(record)
        
        # Calculate overall score
        filter_result["overall_score"] = sum(filter_result["filter_scores"].values())
        
        # Apply minimum quality score threshold
        if filter_result["overall_score"] >= filter_config.get("min_quality_score", 2):
            # Check only critical required filters
            critical_failures = []
            
            # Database ID check (critical)
            if filter_result["filter_scores"]["database_id"] == 0:
                critical_failures.append("No valid database ID")
            
            if not critical_failures:
                filter_result["passes_required_filters"] = True
                filter_result["filter_reasons"] = []
            else:
                filter_result["passes_required_filters"] = False
                filter_result["filter_reasons"] = critical_failures
        else:
            filter_result["passes_required_filters"] = False
            filter_result["filter_reasons"] = ["Overall quality score too low"]
        
        # Add filter result to record
        record["sc_eqtl_filter_result"] = filter_result
        
        # Include records that pass basic filtering
        if filter_result["passes_required_filters"]:
            filtered_records.append(record)
    
    return filtered_records

def assess_records_with_ai(
    records: List[Dict[str, Any]],
    model_client,
    filter_config: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Use AI to assess records for sc-eQTL suitability.
    """
    # Prepare data for AI assessment
    records_for_ai = []
    for record in records:
        record_summary = {
            "accession": record.get("geo_accession") or record.get("run_accession", "Unknown"),
            "title": record.get("title", "")[:200],  # Truncate for efficiency
            "summary": record.get("summary", "")[:500],
            "organism": record.get("organism", ""),
            "tissue": record.get("tissue", ""),
            "platform": record.get("platform", ""),
            "sample_count": record.get("sample_count", 0),
            "library_strategy": record.get("library_strategy", ""),
            "basic_scores": record["sc_eqtl_filter_result"]["filter_scores"]
        }
        records_for_ai.append(record_summary)
    
    # Create AI prompt
    prompt = f"""
You are a professional single-cell genomics data analysis expert. Please evaluate whether the following datasets are suitable for sc-eQTL (single-cell expression quantitative trait loci) analysis.

Evaluation criteria:
1. Species: Prefer human (Homo sapiens)
2. Data type: Single-cell RNA sequencing data, avoid cell lines
3. Sample size: Multiple individuals' samples are better
4. Tissue source: Clear tissue or organ source
5. Technology platform: Modern single-cell sequencing technology
6. Data quality: Sufficient metadata description

Please evaluate the following {len(records_for_ai)} datasets and provide for each dataset:
- suitability_score: Suitability score from 0-10
- confidence: Confidence level from 0-1
- reasons: Evaluation reasons
- sc_eqtl_potential: Potential rating "High"/"Medium"/"Low"

Dataset information:
{records_for_ai}

Please return evaluation results in JSON format:
{{
    "assessments": [
        {{
            "accession": "dataset_id",
            "suitability_score": score,
            "confidence": confidence_level,
            "sc_eqtl_potential": "potential_level",
            "reasons": "evaluation_reasons",
            "recommended": true/false
        }}
    ]
}}
"""
    
    try:
        # Call AI model
        response = model_client.generate_response(prompt)
        
        # Try to parse JSON response
        import json
        try:
            ai_results = json.loads(response)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                ai_results = json.loads(json_match.group())
            else:
                raise ValueError("No valid JSON found in AI response")
        
        # Process AI results
        ai_filtered = []
        ai_assessments = ai_results.get("assessments", [])
        
        for i, record in enumerate(records):
            if i < len(ai_assessments):
                ai_assessment = ai_assessments[i]
                
                # Update record with AI assessment
                record["sc_eqtl_filter_result"]["ai_assessed"] = True
                record["sc_eqtl_filter_result"]["ai_suitability_score"] = ai_assessment.get("suitability_score", 0)
                record["sc_eqtl_filter_result"]["ai_confidence"] = ai_assessment.get("confidence", 0)
                record["sc_eqtl_filter_result"]["ai_potential"] = ai_assessment.get("sc_eqtl_potential", "Low")
                record["sc_eqtl_filter_result"]["ai_reasons"] = ai_assessment.get("reasons", "")
                record["sc_eqtl_filter_result"]["ai_recommended"] = ai_assessment.get("recommended", False)
                
                # Apply AI filtering threshold
                ai_confidence = ai_assessment.get("confidence", 0)
                ai_recommended = ai_assessment.get("recommended", False)
                
                if (ai_confidence >= filter_config.get("ai_confidence_threshold", 0.5) and 
                    ai_recommended):
                    ai_filtered.append(record)
                elif ai_confidence >= 0.3:  # Lower threshold for borderline cases
                    # Include borderline cases for manual review
                    record["sc_eqtl_filter_result"]["needs_manual_review"] = True
                    ai_filtered.append(record)
            else:
                # No AI assessment available, include based on basic filtering
                ai_filtered.append(record)
        
        return ai_filtered
        
    except Exception as e:
        logger.error(f"AI assessment failed: {e}")
        # Return original records if AI fails
        for record in records:
            record["sc_eqtl_filter_result"]["ai_assessed"] = False
            record["sc_eqtl_filter_result"]["ai_error"] = str(e)
        return records

# Lenient versions of filter functions
def check_species_filter_lenient(record: Dict[str, Any], required_species: List[str]) -> int:
    """More lenient species checking."""
    # Handle NULL values safely - use correct field names from database
    organism = (record.get('organism') or '').lower()
    title = (record.get('gse_title') or record.get('study_title') or '').lower()
    summary = (record.get('summary') or record.get('study_abstract') or '').lower()
    organism_ch1 = (record.get('organism_ch1') or '').lower()
    scientific_name = (record.get('scientific_name') or '').lower()
    common_name = (record.get('common_name') or '').lower()
    design_description = (record.get('design_description') or '').lower()
    
    # First, check explicit organism fields (highest priority)
    explicit_organism_fields = [organism, organism_ch1, scientific_name, common_name]
    for field in explicit_organism_fields:
        if field:  # Only check non-empty fields
            # Check for exact human species matches
            if 'homo sapiens' in field or field == 'human':
                return 2
            # Check for other required species
            for species in required_species:
                if species.lower() in field:
                    return 2
    
    # Second, check text fields for human indicators (medium priority)
    text_fields = [title, summary, design_description]
    text_to_check = ' '.join(text_fields)
    
    if text_to_check:
        # More specific human indicators to reduce false positives
        human_indicators = [
            'homo sapiens', 'human patient', 'human subject', 'human clinical',
            'human cancer', 'human tumor', 'human disease', 'human tissue',
            'human blood', 'human pbmc', 'human cell', 'human genome',
            'human breast', 'human lung', 'human liver', 'human brain'
        ]
        
        for identifier in human_indicators:
            if identifier in text_to_check:
                return 2
        
        # Check for required species in text
        for species in required_species:
            species_lower = species.lower()
            if species_lower in text_to_check:
                return 2
    
    # Third, check for broader human context indicators (lower priority)
    broader_indicators = ['patient', 'clinical', 'medical', 'hospital']
    broader_matches = 0
    for indicator in broader_indicators:
        if indicator in text_to_check:
            broader_matches += 1
    
    # If we have multiple broader indicators, it might be human-related
    if broader_matches >= 2:
        return 1  # Lower confidence human match
    
    return 0  # No required species found

def check_cell_line_exclusion_lenient(record: Dict[str, Any]) -> int:
    """More lenient cell line exclusion."""
    cell_line_keywords = [
        'hela', '293t', 'k562', 'jurkat', 'hek293', 'mcf7', 'a549',
        'cell line', 'cell-line', 'immortalized cell', 'transformed cell'
    ]
    
    # Handle NULL values safely - use correct field names from database
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(),
        (record.get('summary') or record.get('study_abstract') or '').lower(),
        (record.get('cell_type') or '').lower(),
        (record.get('source_name') or '').lower(),
        (record.get('source_name_ch1') or '').lower(),
        (record.get('characteristics_ch1') or '').lower(),
        (record.get('sample_name') or '').lower(),
        (record.get('gsm_title') or '').lower()
    ]
    
    for text in text_fields:
        for keyword in cell_line_keywords:
            if keyword in text:
                # Additional check for primary vs cell line
                if 'primary' in text or 'fresh' in text or 'tissue' in text:
                    return 1  # Possibly mixed, not excluded
                return 0  # Likely cell line
    
    return 2  # No cell line detected

def check_publication_info_lenient(record: Dict[str, Any]) -> int:
    """More lenient publication info checking."""
    # Check explicit publication fields - use correct field names
    if (record.get('pmid') or record.get('doi') or 
        record.get('pubmed_id') or record.get('publication')):
        return 2
    
    # Check for publication indicators in text - use correct field names
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(), 
        (record.get('summary') or record.get('study_abstract') or '').lower()
    ]
    pub_indicators = ['pmid', 'doi', 'pubmed', 'published', 'journal', 'paper', 'article', 'study']
    
    for text in text_fields:
        for indicator in pub_indicators:
            if indicator in text:
                return 1
    
    return 0

def check_sample_size_info_lenient(record: Dict[str, Any]) -> int:
    """More lenient sample size checking."""
    # Check explicit sample count fields - use correct field names
    sample_count = safe_int_convert(record.get('sample_count', 0))
    spots = safe_int_convert(record.get('spots', 0))  # SRA field for cell/read count
    
    if sample_count:
        if sample_count >= 50:
            return 2
        elif sample_count >= 10:
            return 1
        else:
            return 0
    
    # For SRA records, spots can indicate data volume
    if spots and spots >= 1000000:  # 1M spots indicates substantial data
        return 1
    
    # Look for sample size in text with more patterns - use correct field names
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(), 
        (record.get('summary') or record.get('study_abstract') or '').lower()
    ]
    
    for text in text_fields:
        # Multiple patterns for sample size
        patterns = [
            r'(\d+)\s*(?:samples?|subjects?|patients?|individuals?|donors?|cases?)',
            r'n\s*=\s*(\d+)',
            r'(\d+)\s*(?:single.?cells?|cells?)',
            r'from\s*(\d+)\s*(?:subjects?|patients?|individuals?)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text)
            if matches:
                max_size = max(int(match) for match in matches)
                if max_size >= 50:
                    return 2
                elif max_size >= 10:
                    return 1
    
    return 0

def check_country_info_lenient(record: Dict[str, Any]) -> int:
    """More lenient country info checking."""
    # Check explicit country fields
    if record.get('country') or record.get('geographic_location'):
        return 2
    
    # Extended country/location keywords
    location_keywords = [
        'usa', 'united states', 'america', 'china', 'uk', 'united kingdom', 'britain',
        'germany', 'japan', 'france', 'canada', 'australia', 'sweden', 'netherlands',
        'italy', 'spain', 'korea', 'israel', 'switzerland', 'denmark', 'norway',
        'university', 'hospital', 'institute', 'medical center', 'clinic'
    ]
    
    # Use correct field names
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(), 
        (record.get('summary') or record.get('study_abstract') or '').lower(),
        (record.get('contributor') or '').lower(),
        (record.get('gse_contact') or '').lower()
    ]
    
    for text in text_fields:
        for keyword in location_keywords:
            if keyword in text:
                return 1
    
    return 0

def check_age_info_lenient(record: Dict[str, Any]) -> int:
    """More lenient age info checking."""
    # Check explicit age fields
    if record.get('age') or record.get('age_range'):
        return 2
    
    # Extended age-related keywords
    age_keywords = [
        'age', 'years old', 'year-old', 'adult', 'pediatric', 'elderly', 'infant',
        'child', 'adolescent', 'neonatal', 'fetal', 'embryonic', 'postnatal',
        'prenatal', 'birth', 'born', 'old', 'young'
    ]
    
    # Use correct field names
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(), 
        (record.get('summary') or record.get('study_abstract') or '').lower(),
        (record.get('characteristics_ch1') or '').lower(),
        (record.get('source_name_ch1') or '').lower()
    ]
    
    for text in text_fields:
        # Age patterns
        age_patterns = [
            r'\b\d+\s*(?:years?\s*old|y\.?o\.?|yr\.?s?)\b',
            r'\b(?:age|aged)\s*\d+',
            r'\b\d+\s*-\s*\d+\s*(?:years?|y\.?o\.?)',
            r'\b(?:week|month|year)s?\s*(?:old|of age)'
        ]
        
        for pattern in age_patterns:
            if re.search(pattern, text):
                return 2
        
        for keyword in age_keywords:
            if keyword in text:
                return 1
    
    return 0



def check_tumor_annotation_lenient(record: Dict[str, Any]) -> int:
    """More lenient tumor annotation checking."""
    # Extended tumor/disease keywords
    disease_keywords = [
        'tumor', 'tumour', 'cancer', 'carcinoma', 'adenocarcinoma', 'malignant',
        'normal', 'healthy', 'control', 'non-tumor', 'benign', 'disease',
        'pathology', 'diagnosis', 'condition', 'disorder', 'syndrome'
    ]
    
    # Use correct field names
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(),
        (record.get('summary') or record.get('study_abstract') or '').lower(),
        (record.get('tissue') or '').lower(),
        (record.get('disease') or '').lower(),
        (record.get('characteristics_ch1') or '').lower(),
        (record.get('source_name_ch1') or '').lower(),
        (record.get('gsm_title') or '').lower()
    ]
    
    for text in text_fields:
        for keyword in disease_keywords:
            if keyword in text:
                return 1
    
    return 0

def check_sequencing_method_lenient(record: Dict[str, Any]) -> int:
    """More lenient sequencing method checking."""
    # Extended sequencing method keywords
    sequencing_methods = [
        '10x genomics', '10x', 'smart-seq', 'smart-seq2', 'drop-seq',
        'cel-seq', 'mars-seq', 'scrb-seq', 'plate-seq', 'well-seq',
        'single-cell', 'single cell', 'sc-rna', 'scrna', 'scrnaseq',
        'rna-seq', 'rnaseq', 'transcriptome', 'expression profiling',
        'illumina', 'nextseq', 'hiseq', 'novaseq', 'sequencing'
    ]
    
    # Handle NULL values safely and use correct field names from database
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(),
        (record.get('summary') or record.get('study_abstract') or '').lower(),
        (record.get('library_strategy') or '').lower(),
        (record.get('platform') or '').lower(),
        (record.get('instrument_model') or '').lower(),
        (record.get('library_source') or '').lower(),
        (record.get('library_construction_protocol') or '').lower(),
        (record.get('design_description') or '').lower(),
        (record.get('overall_design') or '').lower(),
        (record.get('technology') or '').lower(),
        (record.get('gse_type') or '').lower()
    ]
    
    for text in text_fields:
        for method in sequencing_methods:
            if method in text:
                return 2 if 'single' in method or 'sc' in method else 1
    
    return 0

def check_tissue_source_lenient(record: Dict[str, Any]) -> int:
    """More lenient tissue source checking."""
    # Extended tissue/organ keywords
    tissue_keywords = [
        'brain', 'liver', 'heart', 'lung', 'kidney', 'muscle', 'blood',
        'skin', 'bone', 'pancreas', 'stomach', 'intestine', 'colon',
        'breast', 'ovary', 'testis', 'prostate', 'thyroid', 'spleen',
        'tissue', 'organ', 'biopsy', 'sample', 'cell', 'neural',
        'hepatic', 'cardiac', 'pulmonary', 'renal', 'skeletal',
        'pbmc', 'peripheral blood', 'bone marrow', 'lymph node',
        'cortex', 'islet', 'pancreatic', 'hepatocyte', 'neuron',
        'fibroblast', 'endothelial', 'epithelial', 'stem cell'
    ]
    
    # Handle NULL values safely and use correct field names from database
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(),
        (record.get('summary') or record.get('study_abstract') or '').lower(),
        (record.get('source_name_ch1') or '').lower(),
        (record.get('characteristics_ch1') or '').lower(),
        (record.get('sample_name') or '').lower(),
        (record.get('sample_description') or '').lower(),
        (record.get('gsm_description') or '').lower(),
        (record.get('design_description') or '').lower(),
        (record.get('study_description') or '').lower(),
        (record.get('gsm_title') or '').lower()
    ]
    
    for text in text_fields:
        for keyword in tissue_keywords:
            if keyword in text:
                return 1
    
    return 0 

# New intelligent filtering functions with layered strategy

def apply_intelligent_sc_eqtl_filters(
    records: List[Dict[str, Any]],
    filter_config: Optional[Dict[str, Any]] = None,
    use_ai: bool = True,
    ai_batch_size: int = 10
) -> List[Dict[str, Any]]:
    """
    Apply intelligent sc-eQTL filtering with layered strategy and AI assistance.
    
    Strategy:
    1. Critical filters (immediate rejection if failed)
    2. Confidence-based filters (high confidence = accept/reject, low confidence = AI assist)
    3. AI assessment for uncertain cases
    """
    if filter_config is None:
        filter_config = {
            "required_species": ["Homo sapiens", "human"],
            "exclude_cell_lines": True,
            "require_database_id": True,
            "require_publication": False,
            "require_sample_size": False,
            "require_country_info": False,
            "require_age_info": False,
            "require_tumor_annotation": False,
            "require_sequencing_method": False,
            "require_tissue_source": False,
            "min_quality_score": 3,  # Lowered from 2
            "ai_confidence_threshold": 0.6,  # Lowered from 0.7
            "conservative_acceptance_threshold": 4,  # New: accept if score >= 4 when no AI
            "species_confidence_threshold": 0.6,  # Lowered from 0.7
            "cell_line_confidence_threshold": 0.8
        }
    
    logger.info(f"Starting intelligent filtering for {len(records)} records")
    
    # Phase 1: Critical filters (immediate rejection)
    phase1_passed = []
    phase1_stats = {"total": len(records), "critical_failures": 0}
    
    for record in records:
        filter_result = {
            "record": record,
            "phase": "critical_check",
            "passes_critical": True,
            "filter_details": {},
            "rejection_reason": None,
            "confidence_scores": {},
            "needs_ai_review": False
        }
        
        # Critical Filter 1: Database ID (must have valid ID)
        db_id_result = assess_database_id_with_confidence(record)
        filter_result["filter_details"]["database_id"] = db_id_result
        
        if db_id_result["score"] == 0:
            filter_result["passes_critical"] = False
            filter_result["rejection_reason"] = "No valid database ID"
            phase1_stats["critical_failures"] += 1
            record["sc_eqtl_filter_result"] = filter_result
            continue
        
        # Critical Filter 2: Cell Line Exclusion (must not be cell line if required)
        if filter_config["exclude_cell_lines"]:
            cell_line_result = assess_cell_line_with_confidence(record)
            filter_result["filter_details"]["cell_line"] = cell_line_result
            
            if cell_line_result["score"] == 0 and cell_line_result["confidence"] >= 0.8:
                filter_result["passes_critical"] = False
                filter_result["rejection_reason"] = "Contains cell line data (high confidence)"
                phase1_stats["critical_failures"] += 1
                record["sc_eqtl_filter_result"] = filter_result
                continue
        
        # Passed critical filters
        phase1_passed.append(record)
        record["sc_eqtl_filter_result"] = filter_result
    
    logger.info(f"Phase 1 (Critical): {len(phase1_passed)}/{len(records)} records passed")
    
    if not phase1_passed:
        return []
    
    # Phase 2: Confidence-based assessment
    phase2_passed = []
    needs_ai_review = []
    phase2_stats = {"high_confidence_pass": 0, "high_confidence_fail": 0, "needs_ai": 0}
    
    for record in phase1_passed:
        filter_result = record["sc_eqtl_filter_result"]
        filter_result["phase"] = "confidence_assessment"
        
        # Assess all remaining filters with confidence
        assessments = {
            "species": assess_species_with_confidence(record, filter_config["required_species"]),
            "publication": assess_publication_with_confidence(record),
            "sample_size": assess_sample_size_with_confidence(record),
            "country": assess_country_with_confidence(record),
            "age": assess_age_with_confidence(record),
            "tumor": assess_tumor_with_confidence(record),
            "sequencing_method": assess_sequencing_method_with_confidence(record),
            "tissue": assess_tissue_source_with_confidence(record)
        }
        
        # Add cell line assessment if not already done
        if "cell_line" not in filter_result["filter_details"]:
            assessments["cell_line"] = assess_cell_line_with_confidence(record)
        
        filter_result["filter_details"].update(assessments)
        
        # Calculate overall confidence and scores
        total_score = 0
        total_confidence = 0
        uncertain_filters = []
        
        for filter_name, assessment in assessments.items():
            total_score += assessment["score"]
            total_confidence += assessment["confidence"]
            
            # Check if this filter needs AI review
            if assessment["confidence"] < 0.7 and assessment["score"] in [0, 1]:
                uncertain_filters.append(filter_name)
        
        # Add database_id to total score
        total_score += filter_result["filter_details"]["database_id"]["score"]
        
        filter_result["overall_score"] = total_score
        filter_result["average_confidence"] = total_confidence / len(assessments)
        filter_result["uncertain_filters"] = uncertain_filters
        
        # Decision logic
        if len(uncertain_filters) == 0:
            # High confidence decision
            if total_score >= filter_config.get("min_quality_score", 2):
                filter_result["decision"] = "accept_high_confidence"
                phase2_passed.append(record)
                phase2_stats["high_confidence_pass"] += 1
            else:
                filter_result["decision"] = "reject_high_confidence"
                filter_result["rejection_reason"] = f"Low quality score: {total_score}"
                phase2_stats["high_confidence_fail"] += 1
        else:
            # Needs AI review
            filter_result["decision"] = "needs_ai_review"
            filter_result["needs_ai_review"] = True
            needs_ai_review.append(record)
            phase2_stats["needs_ai"] += 1
    
    logger.info(f"Phase 2 (Confidence): {len(phase2_passed)} high-confidence pass, {len(needs_ai_review)} need AI review")
    
    # Phase 3: AI-assisted review for uncertain cases
    if use_ai and needs_ai_review:
        logger.info(f"Phase 3 (AI Review): Processing {len(needs_ai_review)} uncertain records")
        ai_reviewed = process_uncertain_records_with_ai(needs_ai_review, filter_config, ai_batch_size)
        phase2_passed.extend(ai_reviewed)
    else:
        # Without AI, apply conservative filtering to uncertain cases
        logger.warning("ModelClient not available, using conservative filtering")
        for record in needs_ai_review:
            filter_result = record["sc_eqtl_filter_result"]
            conservative_threshold = filter_config.get("conservative_acceptance_threshold", 4)
            if filter_result["overall_score"] >= conservative_threshold:
                filter_result["decision"] = "accept_conservative"
                filter_result["reason"] = f"Score {filter_result['overall_score']} meets conservative threshold"
                phase2_passed.append(record)
            else:
                filter_result["decision"] = "reject_conservative"
                filter_result["rejection_reason"] = f"Score {filter_result['overall_score']} below conservative threshold ({conservative_threshold})"
    
    logger.info(f"Final result: {len(phase2_passed)}/{len(records)} records passed intelligent filtering")
    return phase2_passed


def assess_database_id_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess database ID availability with confidence score."""
    # Check for GEO IDs (multiple possible field names)
    geo_accession = (record.get('gse') or record.get('geo_accession') or '').strip()
    
    # Check for SRA IDs (multiple possible field names)
    sra_accession = (
        record.get('run_accession') or 
        record.get('sra_run_accession') or 
        record.get('study_accession') or 
        record.get('sra_study_accession') or 
        ''
    ).strip()
    
    if geo_accession and geo_accession.startswith('GSE'):
        return {"score": 2, "confidence": 1.0, "reason": f"Valid GEO ID: {geo_accession}", "evidence": geo_accession}
    elif sra_accession and (sra_accession.startswith('SRR') or sra_accession.startswith('SRP') or 
                           sra_accession.startswith('ERR') or sra_accession.startswith('DRR')):
        return {"score": 2, "confidence": 1.0, "reason": f"Valid SRA ID: {sra_accession}", "evidence": sra_accession}
    else:
        return {"score": 0, "confidence": 1.0, "reason": "No valid database ID found", "evidence": f"geo:{geo_accession}, sra:{sra_accession}"}


def assess_species_with_confidence(record: Dict[str, Any], required_species: List[str]) -> Dict[str, Any]:
    """Assess species with confidence scoring."""
    
    # Check taxon_id first (highest confidence)
    taxon_id = str(record.get('taxon_id') or '').strip()
    if taxon_id == '9606':  # Human taxon ID
        return {"score": 2, "confidence": 1.0, "reason": "Human taxon ID (9606)", "evidence": "taxon_id=9606"}
    
    # Explicit organism fields (high confidence)
    # Handle both GEO (organism) and SRA (scientific_name) field names
    scientific_name = (record.get('scientific_name') or '').lower()
    organism = (record.get('organism') or record.get('geo_organism') or record.get('sra_organism') or '').lower()
    organism_ch1 = (record.get('organism_ch1') or '').lower()
    common_name = (record.get('common_name') or '').lower()
    
    explicit_fields = [scientific_name, organism, organism_ch1, common_name]
    
    for field in explicit_fields:
        if field:
            if 'homo sapiens' in field:
                return {"score": 2, "confidence": 1.0, "reason": "Explicit Homo sapiens in organism field", "evidence": field}
            elif field == 'human':
                return {"score": 2, "confidence": 0.9, "reason": "Human in organism field", "evidence": field}
            elif any(species.lower() in field for species in required_species):
                return {"score": 2, "confidence": 0.9, "reason": "Required species in organism field", "evidence": field}
    
    # Text-based detection (medium confidence)
    # Handle both GEO and SRA field names
    title = (
        record.get('gse_title') or record.get('geo_title') or 
        record.get('study_title') or record.get('sra_study_title') or ''
    ).lower()
    summary = (
        record.get('summary') or record.get('geo_summary') or 
        record.get('study_abstract') or ''
    ).lower()
    design_description = (record.get('design_description') or '').lower()
    description = (record.get('description') or '').lower()
    sample_name = (record.get('sample_name') or '').lower()
    
    text_content = f"{title} {summary} {design_description} {description} {sample_name}"
    
    if 'homo sapiens' in text_content:
        return {"score": 2, "confidence": 0.8, "reason": "Homo sapiens in text content", "evidence": "Found in title/summary/description"}
    
    # Human-specific indicators (lower confidence)
    human_indicators = [
        'human patient', 'human subject', 'human clinical', 'human cancer', 
        'human tumor', 'human tissue', 'human blood', 'human cell', 'human'
    ]
    
    for indicator in human_indicators:
        if indicator in text_content:
            return {"score": 2, "confidence": 0.7, "reason": f"Human indicator: {indicator}", "evidence": indicator}
    
    # Medical/clinical context indicators (medium confidence - often human)
    clinical_indicators = ['patient', 'clinical', 'cancer', 'tumor', 'carcinoma', 'adenocarcinoma', 'breast cancer', 'lung cancer']
    clinical_matches = [ind for ind in clinical_indicators if ind in text_content]
    
    if len(clinical_matches) >= 2:
        return {"score": 2, "confidence": 0.6, "reason": f"Multiple clinical indicators (likely human): {clinical_matches}", "evidence": str(clinical_matches)}
    elif len(clinical_matches) == 1:
        return {"score": 1, "confidence": 0.5, "reason": f"Single clinical indicator: {clinical_matches[0]}", "evidence": clinical_matches[0]}
    
    # Check for non-human species indicators
    non_human_indicators = ['mouse', 'rat', 'drosophila', 'zebrafish', 'yeast', 'arabidopsis', 'caenorhabditis', 'escherichia']
    for indicator in non_human_indicators:
        if indicator in text_content:
            return {"score": 0, "confidence": 0.9, "reason": f"Non-human species detected: {indicator}", "evidence": indicator}
    
    return {"score": 0, "confidence": 0.8, "reason": "No human species indicators found", "evidence": "No matches in organism or text fields"}


def assess_cell_line_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess cell line exclusion with confidence."""
    # Known cell line identifiers (high confidence)
    strong_cell_line_keywords = [
        'hela', '293t', 'hek293', 'k562', 'jurkat', 'mcf7', 'a549',
        'cell line', 'cell-line', 'immortalized cell line'
    ]
    
    # Weaker indicators (lower confidence)
    weak_cell_line_keywords = ['immortalized', 'transformed cell']
    
    # Primary tissue indicators (evidence against cell line)
    primary_indicators = ['primary', 'fresh', 'tissue', 'biopsy', 'patient', 'clinical']
    
    text_fields = [
        (record.get('gse_title') or record.get('study_title') or '').lower(),
        (record.get('summary') or record.get('study_abstract') or '').lower(),
        (record.get('source_name_ch1') or '').lower(),
        (record.get('characteristics_ch1') or '').lower(),
        (record.get('gsm_title') or '').lower()
    ]
    
    text_content = ' '.join(text_fields)
    
    # Check for strong cell line indicators
    for keyword in strong_cell_line_keywords:
        if keyword in text_content:
            # Check for primary tissue context
            primary_count = sum(1 for indicator in primary_indicators if indicator in text_content)
            if primary_count >= 2:
                return {"score": 1, "confidence": 0.4, "reason": f"Cell line keyword '{keyword}' but primary context", "evidence": f"{keyword} + primary indicators"}
            else:
                return {"score": 0, "confidence": 0.9, "reason": f"Strong cell line indicator: {keyword}", "evidence": keyword}
    
    # Check for weak cell line indicators
    for keyword in weak_cell_line_keywords:
        if keyword in text_content:
            primary_count = sum(1 for indicator in primary_indicators if indicator in text_content)
            if primary_count >= 2:
                return {"score": 2, "confidence": 0.6, "reason": f"Weak cell line keyword but strong primary context", "evidence": f"{keyword} + primary context"}
            else:
                return {"score": 1, "confidence": 0.3, "reason": f"Weak cell line indicator: {keyword}", "evidence": keyword}
    
    # No cell line indicators found
    primary_count = sum(1 for indicator in primary_indicators if indicator in text_content)
    if primary_count >= 2:
        return {"score": 2, "confidence": 0.8, "reason": "Strong primary tissue indicators", "evidence": f"Primary indicators: {primary_count}"}
    elif primary_count == 1:
        return {"score": 2, "confidence": 0.6, "reason": "Some primary tissue indicators", "evidence": f"Primary indicators: {primary_count}"}
    else:
        return {"score": 2, "confidence": 0.5, "reason": "No cell line indicators found", "evidence": "No cell line keywords detected"}


def assess_publication_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess publication information with confidence."""
    # Explicit publication fields
    if record.get('pmid') or record.get('pubmed_id'):
        pmid = record.get('pmid') or record.get('pubmed_id')
        return {"score": 2, "confidence": 1.0, "reason": f"Has PMID: {pmid}", "evidence": str(pmid)}
    
    if record.get('doi'):
        return {"score": 2, "confidence": 1.0, "reason": f"Has DOI: {record.get('doi')}", "evidence": record.get('doi')}
    
    # Text-based publication indicators
    text_content = f"{record.get('gse_title') or record.get('study_title') or ''} {record.get('summary') or record.get('study_abstract') or ''}".lower()
    
    strong_pub_indicators = ['pmid', 'doi', 'pubmed', 'published in', 'journal']
    for indicator in strong_pub_indicators:
        if indicator in text_content:
            return {"score": 1, "confidence": 0.7, "reason": f"Publication indicator: {indicator}", "evidence": indicator}
    
    weak_pub_indicators = ['published', 'paper', 'article', 'study']
    for indicator in weak_pub_indicators:
        if indicator in text_content:
            return {"score": 1, "confidence": 0.3, "reason": f"Weak publication indicator: {indicator}", "evidence": indicator}
    
    return {"score": 0, "confidence": 0.6, "reason": "No publication information found", "evidence": "No publication indicators"}


def assess_sample_size_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess sample size with confidence."""
    # Explicit sample count
    sample_count = safe_int_convert(record.get('sample_count', 0))
    if sample_count:
        if sample_count >= 100:
            return {"score": 2, "confidence": 1.0, "reason": f"Large sample size: {sample_count}", "evidence": str(sample_count)}
        elif sample_count >= 20:
            return {"score": 1, "confidence": 1.0, "reason": f"Medium sample size: {sample_count}", "evidence": str(sample_count)}
        else:
            return {"score": 0, "confidence": 1.0, "reason": f"Small sample size: {sample_count}", "evidence": str(sample_count)}
    
    # SRA spots as indicator
    spots = safe_int_convert(record.get('spots', 0))
    if spots and spots >= 10000000:  # 10M+ spots suggests substantial data
        return {"score": 1, "confidence": 0.7, "reason": f"High spots count: {spots:,}", "evidence": f"{spots:,} spots"}
    
    # Text-based sample size detection
    text_content = f"{record.get('gse_title') or record.get('study_title') or ''} {record.get('summary') or record.get('study_abstract') or ''}".lower()
    
    import re
    patterns = [
        (r'(\d+)\s*(?:samples?|subjects?|patients?|individuals?|donors?)', 'subjects'),
        (r'n\s*=\s*(\d+)', 'n='),
        (r'(\d+)\s*(?:single.?cells?|cells?)', 'cells'),
        (r'from\s*(\d+)\s*(?:subjects?|patients?)', 'from subjects')
    ]
    
    for pattern, desc in patterns:
        matches = re.findall(pattern, text_content)
        if matches:
            max_size = max(int(match) for match in matches)
            if max_size >= 100:
                return {"score": 2, "confidence": 0.8, "reason": f"Large {desc}: {max_size}", "evidence": f"{desc}={max_size}"}
            elif max_size >= 20:
                return {"score": 1, "confidence": 0.8, "reason": f"Medium {desc}: {max_size}", "evidence": f"{desc}={max_size}"}
            else:
                return {"score": 0, "confidence": 0.8, "reason": f"Small {desc}: {max_size}", "evidence": f"{desc}={max_size}"}
    
    return {"score": 0, "confidence": 0.4, "reason": "No sample size information found", "evidence": "No size indicators"}


def assess_country_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess country information with confidence."""
    # Explicit location fields
    if record.get('country'):
        return {"score": 2, "confidence": 1.0, "reason": f"Country field: {record.get('country')}", "evidence": record.get('country')}
    
    # Institution-based detection
    text_content = f"{record.get('gse_title') or record.get('study_title') or ''} {record.get('contributor') or ''} {record.get('gse_contact') or ''}".lower()
    
    countries = ['usa', 'united states', 'china', 'uk', 'united kingdom', 'germany', 'japan', 'france', 'canada', 'australia']
    for country in countries:
        if country in text_content:
            return {"score": 1, "confidence": 0.7, "reason": f"Country name: {country}", "evidence": country}
    
    institutions = ['university', 'hospital', 'institute', 'medical center', 'clinic']
    for inst in institutions:
        if inst in text_content:
            return {"score": 1, "confidence": 0.4, "reason": f"Institution indicator: {inst}", "evidence": inst}
    
    return {"score": 0, "confidence": 0.5, "reason": "No country information found", "evidence": "No location indicators"}


def assess_age_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess age information with confidence."""
    # Explicit age fields
    if record.get('age') or record.get('age_range'):
        age_info = record.get('age') or record.get('age_range')
        return {"score": 2, "confidence": 1.0, "reason": f"Age field: {age_info}", "evidence": str(age_info)}
    
    # Text-based age detection
    text_content = f"{record.get('gse_title') or record.get('study_title') or ''} {record.get('summary') or record.get('study_abstract') or ''} {record.get('characteristics_ch1') or ''}".lower()
    
    import re
    age_patterns = [
        (r'\b\d+\s*(?:years?\s*old|y\.?o\.?|yr\.?s?)\b', 'age in years'),
        (r'\b(?:age|aged)\s*\d+', 'age mentioned'),
        (r'\b\d+\s*-\s*\d+\s*(?:years?|y\.?o\.?)', 'age range')
    ]
    
    for pattern, desc in age_patterns:
        if re.search(pattern, text_content):
            return {"score": 2, "confidence": 0.9, "reason": f"Age pattern: {desc}", "evidence": desc}
    
    age_keywords = ['adult', 'pediatric', 'elderly', 'infant', 'child', 'adolescent', 'neonatal']
    for keyword in age_keywords:
        if keyword in text_content:
            return {"score": 1, "confidence": 0.7, "reason": f"Age category: {keyword}", "evidence": keyword}
    
    return {"score": 0, "confidence": 0.5, "reason": "No age information found", "evidence": "No age indicators"}


def assess_tumor_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess tumor/disease annotation with confidence."""
    text_content = f"{record.get('gse_title') or record.get('study_title') or ''} {record.get('summary') or record.get('study_abstract') or ''} {record.get('characteristics_ch1') or ''}".lower()
    
    # Strong tumor indicators
    strong_tumor = ['cancer', 'tumor', 'tumour', 'carcinoma', 'adenocarcinoma', 'malignant', 'oncology']
    for indicator in strong_tumor:
        if indicator in text_content:
            return {"score": 1, "confidence": 0.9, "reason": f"Strong tumor indicator: {indicator}", "evidence": indicator}
    
    # Normal/control indicators
    normal_indicators = ['normal', 'healthy', 'control', 'non-tumor', 'benign']
    for indicator in normal_indicators:
        if indicator in text_content:
            return {"score": 1, "confidence": 0.8, "reason": f"Normal tissue indicator: {indicator}", "evidence": indicator}
    
    # General disease indicators
    disease_indicators = ['disease', 'pathology', 'diagnosis', 'condition', 'disorder', 'syndrome']
    for indicator in disease_indicators:
        if indicator in text_content:
            return {"score": 1, "confidence": 0.5, "reason": f"Disease indicator: {indicator}", "evidence": indicator}
    
    return {"score": 0, "confidence": 0.4, "reason": "No tumor/disease annotation found", "evidence": "No disease indicators"}


def assess_sequencing_method_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess sequencing method with confidence."""
    text_content = f"{record.get('gse_title') or record.get('study_title') or ''} {record.get('library_strategy') or ''} {record.get('platform') or ''} {record.get('gse_type') or ''}".lower()
    
    # Single-cell methods (high relevance)
    sc_methods = ['10x genomics', '10x', 'smart-seq', 'drop-seq', 'single-cell', 'single cell', 'sc-rna', 'scrna']
    for method in sc_methods:
        if method in text_content:
            return {"score": 2, "confidence": 0.9, "reason": f"Single-cell method: {method}", "evidence": method}
    
    # RNA-seq methods (medium relevance)
    rna_methods = ['rna-seq', 'rnaseq', 'transcriptome', 'expression profiling']
    for method in rna_methods:
        if method in text_content:
            return {"score": 1, "confidence": 0.7, "reason": f"RNA-seq method: {method}", "evidence": method}
    
    # General sequencing
    seq_indicators = ['illumina', 'nextseq', 'hiseq', 'novaseq', 'sequencing']
    for indicator in seq_indicators:
        if indicator in text_content:
            return {"score": 1, "confidence": 0.5, "reason": f"Sequencing indicator: {indicator}", "evidence": indicator}
    
    return {"score": 0, "confidence": 0.6, "reason": "No sequencing method found", "evidence": "No sequencing indicators"}


def assess_tissue_source_with_confidence(record: Dict[str, Any]) -> Dict[str, Any]:
    """Assess tissue source with confidence."""
    text_content = f"{record.get('gse_title') or record.get('study_title') or ''} {record.get('summary') or record.get('study_abstract') or ''} {record.get('source_name_ch1') or ''} {record.get('characteristics_ch1') or ''}".lower()
    
    # Specific tissues (high confidence)
    specific_tissues = [
        'brain', 'liver', 'heart', 'lung', 'kidney', 'muscle', 'blood',
        'skin', 'bone', 'pancreas', 'stomach', 'intestine', 'colon',
        'breast', 'ovary', 'testis', 'prostate', 'thyroid', 'spleen'
    ]
    
    for tissue in specific_tissues:
        if tissue in text_content:
            return {"score": 1, "confidence": 0.8, "reason": f"Specific tissue: {tissue}", "evidence": tissue}
    
    # Cell types (medium confidence)
    cell_types = [
        'pbmc', 'peripheral blood', 'bone marrow', 'lymph node',
        'hepatocyte', 'neuron', 'fibroblast', 'endothelial', 'epithelial'
    ]
    
    for cell_type in cell_types:
        if cell_type in text_content:
            return {"score": 1, "confidence": 0.7, "reason": f"Cell type: {cell_type}", "evidence": cell_type}
    
    # General indicators
    general_indicators = ['tissue', 'organ', 'biopsy', 'sample', 'cell']
    for indicator in general_indicators:
        if indicator in text_content:
            return {"score": 1, "confidence": 0.4, "reason": f"General tissue indicator: {indicator}", "evidence": indicator}
    
    return {"score": 0, "confidence": 0.5, "reason": "No tissue source found", "evidence": "No tissue indicators"}


def process_uncertain_records_with_ai(
    records: List[Dict[str, Any]], 
    filter_config: Dict[str, Any], 
    batch_size: int = 10
) -> List[Dict[str, Any]]:
    """Process uncertain records with AI assistance."""
    ai_passed = []
    
    try:
        from ..models.client import ModelClient
        model_client = ModelClient()
        
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            logger.info(f"Processing AI batch {i//batch_size + 1}/{(len(records)-1)//batch_size + 1}")
            
            try:
                # Prepare simplified data for AI
                ai_input = []
                for record in batch:
                    filter_result = record["sc_eqtl_filter_result"]
                    
                    record_summary = {
                        "id": record.get("gse") or record.get("run_accession", "Unknown"),
                        "title": (record.get("gse_title") or record.get("study_title", ""))[:200],
                        "summary": (record.get("summary") or record.get("study_abstract", ""))[:300],
                        "organism": record.get("organism") or record.get("scientific_name", ""),
                        "uncertain_filters": filter_result["uncertain_filters"],
                        "filter_details": {k: v for k, v in filter_result["filter_details"].items() if k in filter_result["uncertain_filters"]},
                        "overall_score": filter_result["overall_score"]
                    }
                    ai_input.append(record_summary)
                
                # Create AI prompt
                prompt = create_uncertainty_assessment_prompt(ai_input, filter_config)
                response = model_client.generate_response(prompt)
                
                # Parse AI response
                ai_results = parse_ai_uncertainty_response(response, batch)
                
                for j, record in enumerate(batch):
                    filter_result = record["sc_eqtl_filter_result"]
                    
                    if j < len(ai_results):
                        ai_assessment = ai_results[j]
                        filter_result["ai_assessment"] = ai_assessment
                        
                        if ai_assessment.get("recommended", False):
                            filter_result["decision"] = "accept_ai_assisted"
                            ai_passed.append(record)
                        else:
                            filter_result["decision"] = "reject_ai_assisted"
                            filter_result["rejection_reason"] = ai_assessment.get("reason", "AI assessment negative")
                    else:
                        # Fallback for parsing issues
                        filter_result["decision"] = "reject_ai_error"
                        filter_result["rejection_reason"] = "AI processing error"
                        
            except Exception as e:
                logger.warning(f"AI processing failed for batch {i//batch_size + 1}: {e}")
                # Conservative fallback
                for record in batch:
                    filter_result = record["sc_eqtl_filter_result"]
                    if filter_result["overall_score"] >= 4:  # Higher threshold without AI
                        filter_result["decision"] = "accept_fallback"
                        ai_passed.append(record)
                    else:
                        filter_result["decision"] = "reject_fallback"
                        filter_result["rejection_reason"] = "AI unavailable, conservative filtering"
                        
    except ImportError:
        logger.warning("ModelClient not available, using conservative filtering")
        # Conservative fallback for all records
        for record in records:
            filter_result = record["sc_eqtl_filter_result"]
            if filter_result["overall_score"] >= 5:  # Even higher threshold without AI
                filter_result["decision"] = "accept_no_ai"
                ai_passed.append(record)
            else:
                filter_result["decision"] = "reject_no_ai"
                filter_result["rejection_reason"] = "No AI available, very conservative filtering"
    
    return ai_passed


def create_uncertainty_assessment_prompt(records: List[Dict[str, Any]], filter_config: Dict[str, Any]) -> str:
    """Create AI prompt for assessing uncertain records."""
    prompt = f"""You are an expert in single-cell genomics data curation for sc-eQTL analysis.

TASK: Assess {len(records)} datasets with uncertain filter results to determine their suitability for sc-eQTL studies.

CONTEXT: These records have passed critical filters (valid database IDs, not obvious cell lines) but have uncertain scores on some criteria. Your expertise is needed to make the final determination.

DATASETS TO ASSESS:
"""
    
    for i, record in enumerate(records, 1):
        prompt += f"""
Dataset {i}:
- ID: {record['id']}
- Title: {record['title']}
- Summary: {record['summary']}
- Organism: {record['organism']}
- Current Score: {record['overall_score']}
- Uncertain Filters: {record['uncertain_filters']}
- Filter Details: {record['filter_details']}
"""
    
    prompt += f"""

ASSESSMENT CRITERIA for sc-eQTL suitability:
1. Must be human data (Homo sapiens)
2. Should not be immortalized cell lines
3. Preferably has substantial sample size
4. Single-cell or bulk RNA-seq data
5. Clear tissue/cell type annotation
6. Disease context helpful but not required

Please provide your assessment in JSON format:
{{
  "assessments": [
    {{
      "dataset_id": "ID",
      "recommended": true/false,
      "confidence": 0.0-1.0,
      "reason": "Brief explanation",
      "sc_eqtl_potential": "High/Medium/Low"
    }},
    ...
  ]
}}

Focus on practical utility for sc-eQTL analysis. Be reasonably permissive for borderline cases that could contribute valuable data.
"""
    
    return prompt


def parse_ai_uncertainty_response(response: str, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Parse AI response for uncertainty assessment."""
    try:
        import json
        import re
        
        # Try to parse JSON response
        try:
            ai_results = json.loads(response)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                ai_results = json.loads(json_match.group())
            else:
                raise ValueError("No valid JSON found in AI response")
        
        assessments = ai_results.get("assessments", [])
        return assessments
        
    except Exception as e:
        logger.error(f"Failed to parse AI response: {e}")
        # Return conservative fallback
        return [{"recommended": False, "confidence": 0.0, "reason": "AI parsing error"} for _ in records]